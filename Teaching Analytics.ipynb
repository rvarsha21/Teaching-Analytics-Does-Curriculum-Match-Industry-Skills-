{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1zTIshZjeAw",
    "outputId": "a311f2a4-343f-4d9e-b0bd-7e5e3cfe6d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.13/site-packages (4.35.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.13/site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zTuN0G3RYGnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INDEED JOB SCRAPER\n",
      "============================================================\n",
      "\n",
      "Default designations:\n",
      "  1. Business Analyst\n",
      "  2. Data Analyst\n",
      "  3. Data Scientist\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Use default designations? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Designations to scrape:\n",
      "  1. Business Analyst\n",
      "  2. Data Analyst\n",
      "  3. Data Scientist\n",
      "============================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number of jobs to scrape per designation (e.g., 10, 50, 500):  500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting scrape: 500 jobs per designation\n",
      "Total jobs to collect: 1500\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Starting multi-category scraping...\n",
      "Target: 500 jobs per designation\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Searching for Business Analyst (Need 500 more) ---\n",
      "  Error extracting description: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"jobDescriptionText\"]\"}\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f7188c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 366336\n",
      "4   chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "5   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "6   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "7   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "8   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "9   chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "10  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "11  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "12  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "13  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "14  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  âŠ— No category match - skipping\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "  Error processing job card: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f3ae08 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 142460\n",
      "4   chromedriver                        0x0000000100f39ee4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 138584\n",
      "5   chromedriver                        0x0000000100f30968 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 100316\n",
      "6   chromedriver                        0x0000000100f2f170 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 94180\n",
      "7   chromedriver                        0x0000000100f32314 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 106888\n",
      "8   chromedriver                        0x0000000100f323cc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 107072\n",
      "9   chromedriver                        0x0000000100f73200 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 372852\n",
      "10  chromedriver                        0x0000000100f679ec _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 325728\n",
      "11  chromedriver                        0x0000000100f67504 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 324472\n",
      "12  chromedriver                        0x0000000100fb2d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "13  chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "14  chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "15  chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "16  chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "17  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "18  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "19  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "20  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "21  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "22  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "23  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "\n",
      "--- Searching for Data Analyst (Need 500 more) ---\n",
      "  No more jobs found on page 1\n",
      "\n",
      "--- Searching for Data Scientist (Need 500 more) ---\n",
      "  No more jobs found on page 1\n",
      "\n",
      "============================================================\n",
      "PROGRESS REPORT:\n",
      "  Business Analyst: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "  Data Analyst: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "  Data Scientist: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Searching for Business Analyst (Need 500 more) ---\n",
      "  Error loading page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=142.0.7444.176)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f03d08 chromedriver + 146696\n",
      "4   chromedriver                        0x0000000100f99c1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 531088\n",
      "5   chromedriver                        0x0000000100fb28b4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 632616\n",
      "6   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "7   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "8   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "9   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "10  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "11  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "12  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "13  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "14  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "15  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "\n",
      "--- Searching for Data Analyst (Need 500 more) ---\n",
      "  Error loading page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=142.0.7444.176)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f03d08 chromedriver + 146696\n",
      "4   chromedriver                        0x0000000100f99c1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 531088\n",
      "5   chromedriver                        0x0000000100fb28b4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 632616\n",
      "6   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "7   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "8   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "9   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "10  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "11  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "12  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "13  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "14  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "15  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "\n",
      "--- Searching for Data Scientist (Need 500 more) ---\n",
      "  Error loading page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=142.0.7444.176)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f03d08 chromedriver + 146696\n",
      "4   chromedriver                        0x0000000100f99c1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 531088\n",
      "5   chromedriver                        0x0000000100fb28b4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 632616\n",
      "6   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "7   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "8   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "9   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "10  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "11  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "12  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "13  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "14  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "15  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "\n",
      "============================================================\n",
      "PROGRESS REPORT:\n",
      "  Business Analyst: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "  Data Analyst: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "  Data Scientist: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0/500 (0.0%)\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Searching for Business Analyst (Need 500 more) ---\n",
      "  Error loading page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=142.0.7444.176)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f03d08 chromedriver + 146696\n",
      "4   chromedriver                        0x0000000100f99c1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 531088\n",
      "5   chromedriver                        0x0000000100fb28b4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 632616\n",
      "6   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "7   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "8   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "9   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "10  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "11  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "12  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "13  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "14  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "15  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n",
      "\n",
      "--- Searching for Data Analyst (Need 500 more) ---\n",
      "  Error loading page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=142.0.7444.176)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010141aecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x0000000101412b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x0000000100f2a2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x0000000100f03d08 chromedriver + 146696\n",
      "4   chromedriver                        0x0000000100f99c1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 531088\n",
      "5   chromedriver                        0x0000000100fb28b4 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 632616\n",
      "6   chromedriver                        0x0000000100f65ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "7   chromedriver                        0x00000001013de0c8 cxxbridge1$str$ptr + 2692164\n",
      "8   chromedriver                        0x00000001013e18dc cxxbridge1$str$ptr + 2706520\n",
      "9   chromedriver                        0x00000001013be84c cxxbridge1$str$ptr + 2563016\n",
      "10  chromedriver                        0x00000001013e21b4 cxxbridge1$str$ptr + 2708784\n",
      "11  chromedriver                        0x00000001013b00f4 cxxbridge1$str$ptr + 2503792\n",
      "12  chromedriver                        0x0000000101401498 cxxbridge1$str$ptr + 2836500\n",
      "13  chromedriver                        0x000000010140161c cxxbridge1$str$ptr + 2836888\n",
      "14  chromedriver                        0x00000001014127d8 cxxbridge1$str$ptr + 2906964\n",
      "15  libsystem_pthread.dylib             0x000000018176bc0c _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000181766b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 552\u001b[0m\n\u001b[1;32m    547\u001b[0m         scraper\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# Option 1: Interactive mode - will prompt for designations and job count\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 526\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(jobs_per_designation, designations)\u001b[0m\n\u001b[1;32m    522\u001b[0m scraper \u001b[38;5;241m=\u001b[39m IndeedJobScraper(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Set to True for headless mode\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# Scrape jobs across all designations simultaneously\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     total_collected \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39msearch_jobs(designations, jobs_per_designation)\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# Save all data to CSV\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mIndeedJobScraper.search_jobs\u001b[0;34m(self, designations, num_jobs_per_designation)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Search for this designation\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_designation_page(designation, designations, num_jobs_per_designation, page)\n\u001b[0;32m---> 63\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Show progress\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress(designations, num_jobs_per_designation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "\n",
    "class IndeedJobScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"Initialize the scraper with Chrome options\"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.job_data = []\n",
    "        self.jobs_by_category = {}  # Track jobs collected per category\n",
    "\n",
    "    def search_jobs(self, designations, num_jobs_per_designation):\n",
    "        \"\"\"Search for jobs on Indeed across multiple designations\n",
    "\n",
    "        Args:\n",
    "            designations: List of job designations to search for\n",
    "            num_jobs_per_designation: Number of jobs to collect per designation\n",
    "        \"\"\"\n",
    "        # Initialize tracking for each designation\n",
    "        for designation in designations:\n",
    "            self.jobs_by_category[designation] = 0\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting multi-category scraping...\")\n",
    "        print(f\"Target: {num_jobs_per_designation} jobs per designation\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Continue scraping until all categories are filled\n",
    "        max_pages = 50  # Safety limit\n",
    "        page = 0\n",
    "\n",
    "        while not self.all_categories_filled(designations, num_jobs_per_designation) and page < max_pages:\n",
    "            page += 1\n",
    "\n",
    "            # Cycle through each designation to find jobs\n",
    "            for designation in designations:\n",
    "                # Check if this category already has enough jobs\n",
    "                if self.jobs_by_category[designation] >= num_jobs_per_designation:\n",
    "                    continue\n",
    "\n",
    "                jobs_needed = num_jobs_per_designation - self.jobs_by_category[designation]\n",
    "                print(f\"\\n--- Searching for {designation} (Need {jobs_needed} more) ---\")\n",
    "\n",
    "                # Search for this designation\n",
    "                self.scrape_designation_page(designation, designations, num_jobs_per_designation, page)\n",
    "\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "\n",
    "            # Show progress\n",
    "            self.show_progress(designations, num_jobs_per_designation)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Scraping completed!\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return sum(self.jobs_by_category.values())\n",
    "\n",
    "    def scrape_designation_page(self, current_designation, all_designations, target_per_category, page_num):\n",
    "        \"\"\"Scrape a single page for a specific designation\"\"\"\n",
    "        base_url = \"https://www.indeed.com/jobs\"\n",
    "        start = page_num * 10\n",
    "\n",
    "        # Construct search URL\n",
    "        search_url = f\"{base_url}?q={current_designation.replace(' ', '+')}&l=United States&start={start}\"\n",
    "\n",
    "        try:\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "\n",
    "            # Find all job cards\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "\n",
    "            if not job_cards:\n",
    "                print(f\"  No more jobs found on page {page_num}\")\n",
    "                return\n",
    "\n",
    "            for card in job_cards:\n",
    "                # Check if all categories are filled\n",
    "                if self.all_categories_filled(all_designations, target_per_category):\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    # Click on job card\n",
    "                    card.click()\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "\n",
    "                    # Extract job details (without pre-assigned category)\n",
    "                    job_info = self.extract_job_details()\n",
    "\n",
    "                    if job_info:\n",
    "                        # Intelligently categorize the job based on its title\n",
    "                        matched_category = self.match_job_to_category(job_info['job_title'], all_designations)\n",
    "\n",
    "                        if matched_category and self.jobs_by_category[matched_category] < target_per_category:\n",
    "                            job_info['search_category'] = matched_category\n",
    "\n",
    "                            # Validate job data\n",
    "                            validation_result = self.validate_job_data(job_info)\n",
    "\n",
    "                            if validation_result['valid']:\n",
    "                                self.job_data.append(job_info)\n",
    "                                self.jobs_by_category[matched_category] += 1\n",
    "                                salary_display = f\"${job_info['salary']}\" if job_info['salary'] != 'Not specified' else 'No salary info'\n",
    "                                print(f\"  âœ“ [{matched_category}] {self.jobs_by_category[matched_category]}/{target_per_category} - {job_info['job_title']} at {job_info['company_name']} ({salary_display})\")\n",
    "                            else:\n",
    "                                print(f\"  âœ— Skipped - {validation_result['reason']}\")\n",
    "                        else:\n",
    "                            if matched_category:\n",
    "                                print(f\"  âŠ— [{matched_category}] already filled - skipping\")\n",
    "                            else:\n",
    "                                print(f\"  âŠ— No category match - skipping\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing job card: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading page: {str(e)}\")\n",
    "\n",
    "    def match_job_to_category(self, job_title, designations):\n",
    "        \"\"\"Match a job title to one of the target designations\n",
    "\n",
    "        Returns the best matching designation or None\n",
    "        \"\"\"\n",
    "        if not job_title or job_title == 'Not specified':\n",
    "            return None\n",
    "\n",
    "        job_title_lower = job_title.lower()\n",
    "\n",
    "        # Create matching keywords for each designation\n",
    "        designation_keywords = {}\n",
    "        for designation in designations:\n",
    "            # Extract key terms from designation\n",
    "            terms = designation.lower().split()\n",
    "            designation_keywords[designation] = terms\n",
    "\n",
    "        # Score each designation based on keyword matches\n",
    "        scores = {}\n",
    "        for designation, keywords in designation_keywords.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in job_title_lower)\n",
    "            if score > 0:\n",
    "                scores[designation] = score\n",
    "\n",
    "        # Return designation with highest score\n",
    "        if scores:\n",
    "            return max(scores, key=scores.get)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def all_categories_filled(self, designations, target):\n",
    "        \"\"\"Check if all categories have reached their target\"\"\"\n",
    "        return all(self.jobs_by_category.get(des, 0) >= target for des in designations)\n",
    "\n",
    "    def show_progress(self, designations, target):\n",
    "        \"\"\"Display progress for all categories\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PROGRESS REPORT:\")\n",
    "        for designation in designations:\n",
    "            count = self.jobs_by_category.get(designation, 0)\n",
    "            percentage = (count / target) * 100\n",
    "            bar = 'â–ˆ' * int(percentage / 5) + 'â–‘' * (20 - int(percentage / 5))\n",
    "            print(f\"  {designation}: [{bar}] {count}/{target} ({percentage:.1f}%)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    def extract_job_details(self):\n",
    "        \"\"\"Extract detailed information from job posting\"\"\"\n",
    "        job_info = {\n",
    "            'search_category': '',  # Will be assigned later based on matching\n",
    "            'job_title': '',\n",
    "            'company_name': '',\n",
    "            'requirements_and_qualifications': '',\n",
    "            'responsibilities': '',\n",
    "            'salary': 'Not specified',\n",
    "            'experience_years': 'Not specified'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Job Title (Exact designation including levels)\n",
    "            try:\n",
    "                job_title = self.driver.find_element(By.CSS_SELECTOR, 'h1.jobsearch-JobInfoHeader-title')\n",
    "                job_info['job_title'] = job_title.text.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    job_title = self.driver.find_element(By.CSS_SELECTOR, '[data-testid=\"jobsearch-JobInfoHeader-title\"]')\n",
    "                    job_info['job_title'] = job_title.text.strip()\n",
    "                except:\n",
    "                    job_info['job_title'] = 'Not specified'\n",
    "\n",
    "            # Company Name\n",
    "            try:\n",
    "                company = self.driver.find_element(By.CSS_SELECTOR, '[data-testid=\"inlineHeader-companyName\"]')\n",
    "                job_info['company_name'] = company.text.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    company = self.driver.find_element(By.CSS_SELECTOR, '.jobsearch-InlineCompanyRating')\n",
    "                    job_info['company_name'] = company.text.split('\\n')[0].strip()\n",
    "                except:\n",
    "                    job_info['company_name'] = 'Not specified'\n",
    "\n",
    "            # Salary - Check multiple locations\n",
    "            salary_found = False\n",
    "\n",
    "            # Method 1: Check the salary header element\n",
    "            try:\n",
    "                salary = self.driver.find_element(By.CSS_SELECTOR, '[data-testid=\"jobsearch-jobDescriptionHeader-salaryDescription\"]')\n",
    "                job_info['salary'] = salary.text.strip()\n",
    "                salary_found = True\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Method 2: Check for salary in the job attributes section\n",
    "            if not salary_found:\n",
    "                try:\n",
    "                    salary = self.driver.find_element(By.CSS_SELECTOR, '#salaryInfoAndJobType')\n",
    "                    salary_text = salary.text.strip()\n",
    "                    if salary_text and ('a year' in salary_text.lower() or 'an hour' in salary_text.lower() or '$' in salary_text):\n",
    "                        job_info['salary'] = salary_text.split('\\n')[0].strip()\n",
    "                        salary_found = True\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Method 3: Check salary guide/info sections\n",
    "            if not salary_found:\n",
    "                try:\n",
    "                    salary_elements = self.driver.find_elements(By.CSS_SELECTOR, '[class*=\"salary\"], [class*=\"Salary\"]')\n",
    "                    for element in salary_elements:\n",
    "                        text = element.text.strip()\n",
    "                        if text and ('$' in text or 'year' in text.lower() or 'hour' in text.lower()):\n",
    "                            job_info['salary'] = text.split('\\n')[0].strip()\n",
    "                            salary_found = True\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Method 4: Extract from job description text\n",
    "            if not salary_found:\n",
    "                try:\n",
    "                    description = self.driver.find_element(By.ID, 'jobDescriptionText')\n",
    "                    salary_from_description = self.extract_salary_from_text(description.text)\n",
    "                    if salary_from_description:\n",
    "                        job_info['salary'] = salary_from_description\n",
    "                        salary_found = True\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Job Description (contains requirements, responsibilities, etc.)\n",
    "            try:\n",
    "                description = self.driver.find_element(By.ID, 'jobDescriptionText')\n",
    "                full_text = description.text.lower()\n",
    "\n",
    "                # Extract Requirements and Qualifications (combined)\n",
    "                requirements = self.extract_section(full_text,\n",
    "                    ['requirement', 'qualification', 'required skills', 'must have', 'eligibility',\n",
    "                     'education', 'degree', 'bachelor', 'master', 'experience', 'skills'])\n",
    "                job_info['requirements_and_qualifications'] = requirements if requirements else 'Not specified'\n",
    "\n",
    "                # Extract Responsibilities\n",
    "                responsibilities = self.extract_section(full_text,\n",
    "                    ['responsibilit', 'duties', 'you will', 'role', 'job duties'])\n",
    "                job_info['responsibilities'] = responsibilities if responsibilities else 'Not specified'\n",
    "\n",
    "                # Extract Experience\n",
    "                experience = self.extract_experience(full_text)\n",
    "                if experience:\n",
    "                    job_info['experience_years'] = experience\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error extracting description: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in extract_job_details: {str(e)}\")\n",
    "\n",
    "        return job_info\n",
    "\n",
    "    def extract_section(self, text, keywords):\n",
    "        \"\"\"Extract specific sections from job description\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        section_lines = []\n",
    "        capturing = False\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            # Check if line contains any keyword\n",
    "            if any(keyword in line.lower() for keyword in keywords):\n",
    "                capturing = True\n",
    "                section_lines.append(line)\n",
    "                # Get next several lines\n",
    "                for j in range(i+1, min(i+10, len(lines))):\n",
    "                    if lines[j].strip() and not any(stop in lines[j].lower()\n",
    "                        for stop in ['about', 'company', 'benefits', 'equal opportunity']):\n",
    "                        section_lines.append(lines[j])\n",
    "                    else:\n",
    "                        break\n",
    "                break\n",
    "\n",
    "        return ' '.join(section_lines).strip()[:800] if section_lines else ''\n",
    "\n",
    "    def extract_experience(self, text):\n",
    "        \"\"\"Extract years of experience from text\"\"\"\n",
    "        patterns = [\n",
    "            r'(\\d+)\\+?\\s*(?:years?|yrs?)\\s*(?:of)?\\s*experience',\n",
    "            r'experience[:\\s]+(\\d+)\\+?\\s*(?:years?|yrs?)',\n",
    "            r'minimum\\s*(?:of)?\\s*(\\d+)\\s*(?:years?|yrs?)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return f\"{match.group(1)}+ years\"\n",
    "\n",
    "        return 'Not specified'\n",
    "\n",
    "    def extract_salary_from_text(self, text):\n",
    "        \"\"\"Extract salary information from job description text\"\"\"\n",
    "        # Common salary patterns\n",
    "        patterns = [\n",
    "            r'\\$[\\d,]+\\.?\\d*\\s*[-â€“to]+\\s*\\$[\\d,]+\\.?\\d*\\s*(?:per|/)?\\s*(?:year|yr|annum|hour|hr)?',  # $80,000 - $100,000 per year\n",
    "            r'\\$[\\d,]+\\.?\\d*[kK]\\s*[-â€“to]+\\s*\\$[\\d,]+\\.?\\d*[kK]\\s*(?:per|/)?\\s*(?:year|yr|annum)?',  # $80K - $100K\n",
    "            r'\\$[\\d,]+\\.?\\d*\\s*(?:per|/)?\\s*(?:year|yr|annum|hour|hr)',  # $80,000 per year\n",
    "            r'salary\\s*:?\\s*\\$[\\d,]+\\.?\\d*[kK]?',  # Salary: $80,000\n",
    "            r'compensation\\s*:?\\s*\\$[\\d,]+\\.?\\d*[kK]?',  # Compensation: $80K\n",
    "        ]\n",
    "\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines[:30]:  # Check first 30 lines\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, line, re.IGNORECASE)\n",
    "                if match:\n",
    "                    salary_text = match.group(0).strip()\n",
    "                    # Clean up the salary text\n",
    "                    if len(salary_text) > 5 and len(salary_text) < 100:\n",
    "                        return salary_text\n",
    "\n",
    "        return None\n",
    "\n",
    "    def validate_job_data(self, job_info):\n",
    "        \"\"\"Validate if job data meets quality and eligibility criteria\n",
    "\n",
    "        Returns dict with 'valid' (bool) and 'reason' (str) keys\n",
    "        \"\"\"\n",
    "        # Fields to check for data quality (excluding search_category and job_title)\n",
    "        fields_to_check = [\n",
    "            'company_name',\n",
    "            'requirements_and_qualifications',\n",
    "            'responsibilities',\n",
    "            'salary',\n",
    "            'experience_years'\n",
    "        ]\n",
    "\n",
    "        # Count how many fields are \"Not specified\"\n",
    "        not_specified_count = sum(\n",
    "            1 for field in fields_to_check\n",
    "            if job_info.get(field, '').strip() == 'Not specified' or not job_info.get(field, '').strip()\n",
    "        )\n",
    "\n",
    "        # Check if more than 2 fields are missing\n",
    "        if not_specified_count > 2:\n",
    "            return {'valid': False, 'reason': f'Insufficient data ({not_specified_count} fields missing, max 2 allowed)'}\n",
    "\n",
    "        # Check if Master's degree is in requirements/qualifications\n",
    "        requirements_qual = job_info.get('requirements_and_qualifications', '').lower()\n",
    "\n",
    "        # If requirements/qualifications is \"Not specified\", reject it\n",
    "        if requirements_qual == 'not specified' or not requirements_qual.strip():\n",
    "            return {'valid': False, 'reason': 'Requirements and qualifications not specified'}\n",
    "\n",
    "        # Check if \"master\" or \"masters\" is mentioned in requirements/qualifications\n",
    "        has_masters = any(term in requirements_qual for term in [\n",
    "            'master', 'masters', 'ms ', 'm.s.', 'graduate degree', 'advanced degree'\n",
    "        ])\n",
    "\n",
    "        if not has_masters:\n",
    "            return {'valid': False, 'reason': \"Master's degree not in requirements\"}\n",
    "\n",
    "        return {'valid': True, 'reason': ''}\n",
    "\n",
    "    def is_valid_job_data(self, job_info):\n",
    "        \"\"\"Legacy method - kept for backward compatibility\"\"\"\n",
    "        result = self.validate_job_data(job_info)\n",
    "        return result['valid']\n",
    "\n",
    "    def save_to_csv(self, filename='indeed_jobs.csv'):\n",
    "        \"\"\"Save scraped data to CSV\"\"\"\n",
    "        if self.job_data:\n",
    "            df = pd.DataFrame(self.job_data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Data saved to {filename}\")\n",
    "            print(f\"Total jobs scraped: {len(self.job_data)}\")\n",
    "\n",
    "            # Show data quality statistics\n",
    "            print(f\"\\nData Quality Report:\")\n",
    "            print(f\"  All jobs include Master's degree in requirements âœ“\")\n",
    "            for col in ['company_name', 'requirements_and_qualifications', 'responsibilities', 'salary', 'experience_years']:\n",
    "                not_specified = (df[col] == 'Not specified').sum()\n",
    "                percentage = (not_specified / len(df)) * 100\n",
    "                print(f\"  {col}: {len(df) - not_specified}/{len(df)} complete ({percentage:.1f}% missing)\")\n",
    "\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data to save.\")\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "def main(jobs_per_designation=None, designations=None):\n",
    "    \"\"\"Main execution function\n",
    "\n",
    "    Args:\n",
    "        jobs_per_designation: Number of jobs to scrape per designation.\n",
    "                             If None, will prompt user for input.\n",
    "        designations: List of job designations to scrape.\n",
    "                     If None, will prompt user for input.\n",
    "    \"\"\"\n",
    "    # Get job designations\n",
    "    if designations is None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INDEED JOB SCRAPER\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nDefault designations:\")\n",
    "        default_designations = [\n",
    "            'Business Analyst',\n",
    "            'Data Analyst',\n",
    "            'Data Scientist'\n",
    "        ]\n",
    "        for i, des in enumerate(default_designations, 1):\n",
    "            print(f\"  {i}. {des}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        use_default = input(\"\\nUse default designations? (yes/no): \").lower()\n",
    "\n",
    "        if use_default in ['yes', 'y']:\n",
    "            designations = default_designations\n",
    "        else:\n",
    "            designations = []\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"Enter job designations to scrape (one per line)\")\n",
    "            print(\"Note: The scraper will automatically capture all levels\")\n",
    "            print(\"(Junior, Senior, I, II, III, Lead, Principal, etc.)\")\n",
    "            print(\"Type 'done' when finished\")\n",
    "            print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "            while True:\n",
    "                designation = input(f\"Enter designation #{len(designations) + 1} (or 'done'): \").strip()\n",
    "\n",
    "                if designation.lower() == 'done':\n",
    "                    if len(designations) == 0:\n",
    "                        print(\"âš ï¸  You must enter at least one designation.\")\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                if designation:\n",
    "                    designations.append(designation)\n",
    "                    print(f\"âœ“ Added: {designation}\")\n",
    "                else:\n",
    "                    print(\"âš ï¸  Please enter a valid designation.\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Designations to scrape:\")\n",
    "        for i, des in enumerate(designations, 1):\n",
    "            print(f\"  {i}. {des}\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INDEED JOB SCRAPER\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nDesignations to scrape:\")\n",
    "        for i, des in enumerate(designations, 1):\n",
    "            print(f\"  {i}. {des}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    # Get number of jobs per designation\n",
    "    if jobs_per_designation is None:\n",
    "        print(\"\\n\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                jobs_input = input(\"Enter number of jobs to scrape per designation (e.g., 10, 50, 500): \")\n",
    "                jobs_per_designation = int(jobs_input)\n",
    "\n",
    "                if jobs_per_designation <= 0:\n",
    "                    print(\"Please enter a positive number.\")\n",
    "                    continue\n",
    "                elif jobs_per_designation > 1000:\n",
    "                    confirm = input(f\"âš ï¸  You want to scrape {jobs_per_designation} jobs per designation ({jobs_per_designation * len(designations)} total). This may take a long time. Continue? (yes/no): \")\n",
    "                    if confirm.lower() not in ['yes', 'y']:\n",
    "                        continue\n",
    "\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting scrape: {jobs_per_designation} jobs per designation\")\n",
    "        print(f\"Total jobs to collect: {jobs_per_designation * len(designations)}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Scraping {jobs_per_designation} jobs per designation\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Initialize scraper\n",
    "    scraper = IndeedJobScraper(headless=False)  # Set to True for headless mode\n",
    "\n",
    "    try:\n",
    "        # Scrape jobs across all designations simultaneously\n",
    "        total_collected = scraper.search_jobs(designations, jobs_per_designation)\n",
    "\n",
    "        # Save all data to CSV\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'indeed_jobs_{jobs_per_designation}per_{timestamp}.csv'\n",
    "        df = scraper.save_to_csv(filename)\n",
    "\n",
    "        # Display summary\n",
    "        if df is not None:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SCRAPING SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"\\nJobs by Search Category:\")\n",
    "            print(df['search_category'].value_counts())\n",
    "            print(f\"\\nSample Job Titles Found:\")\n",
    "            print(df['job_title'].value_counts().head(10))\n",
    "            print(f\"\\nSample Data:\")\n",
    "            print(df.head(3)[['search_category', 'job_title', 'company_name', 'salary', 'experience_years']])\n",
    "\n",
    "    finally:\n",
    "        # Close browser\n",
    "        scraper.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Interactive mode - will prompt for designations and job count\n",
    "    main()\n",
    "\n",
    "    # Option 2: Specify designations directly\n",
    "    # custom_designations = ['Financial Analyst', 'Machine Learning Engineer', 'Product Manager']\n",
    "    # main(jobs_per_designation=50, designations=custom_designations)\n",
    "\n",
    "    # Option 3: Use defaults but specify job count\n",
    "    # main(jobs_per_designation=10)  # Uses default designations with 10 jobs each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZbZVYP5LlgN"
   },
   "source": [
    "Skills extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnr-pVVmLkMl"
   },
   "outputs": [],
   "source": [
    "# step3_skills_only.py\n",
    "# Inputs:\n",
    "#   /mnt/data/MSBA-top5.csv            (curricula; must have a university-like column)\n",
    "#   /mnt/data/cleaned_job_data.csv     (jobs; should have role/title and job description)\n",
    "# Outputs:\n",
    "#   /mnt/data/skills_from_curriculum.csv   -> columns: university, skill, count\n",
    "#   /mnt/data/skills_from_jobs.csv         -> columns: role, skill, count\n",
    "#\n",
    "# No displays, no cosine, no fluff.\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CURR = \"/Users/varsha/Desktop/Fall sem/Unstructured data/Project/MSBA_top_10.csv\"\n",
    "JOBS = \"cleaned_job_data.csv\"\n",
    "OUT  = Path('/Users/varsha/Desktop/Fall sem/Unstructured data/Project')\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def pick_cols(df, hints):\n",
    "    return [c for c in df.columns if any(h in c.lower() for h in hints)]\n",
    "\n",
    "def collapse_text(df, cols):\n",
    "    if not cols:\n",
    "        return pd.Series([\"\"] * len(df))\n",
    "    block = df[cols].astype(str).fillna(\"\")\n",
    "    return block.apply(lambda r: \" \".join([x for x in r if x.strip()]), axis=1)\n",
    "\n",
    "def norm_variant(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[-_/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# ---------------- expanded skill vocab ----------------\n",
    "RAW_SKILLS = [\n",
    "    # Languages & query\n",
    "    \"python\",\"r\",\"java\",\"c\",\"c++\",\"c#\",\"scala\",\"julia\",\"matlab\",\"sql\",\"tsql\",\"nosql\",\n",
    "    # BI & viz\n",
    "    \"tableau\",\"power bi\",\"looker\",\"qlik\",\"excel\",\"vba\",\"superset\",\"metabase\",\n",
    "    # Py data & viz\n",
    "    \"pandas\",\"numpy\",\"scipy\",\"statsmodels\",\"matplotlib\",\"plotly\",\"bokeh\",\"seaborn\",\n",
    "    # ML/DL libs\n",
    "    \"scikit-learn\",\"sklearn\",\"xgboost\",\"lightgbm\",\"catboost\",\"tensorflow\",\"pytorch\",\"keras\",\"fastai\",\"prophet\",\"gluonts\",\n",
    "    # NLP/CV\n",
    "    \"spacy\",\"nltk\",\"transformers\",\"huggingface\",\"gensim\",\"word2vec\",\"bert\",\"roberta\",\"gpt\",\"opencv\",\n",
    "    # Big data\n",
    "    \"spark\",\"pyspark\",\"hadoop\",\"mapreduce\",\"hive\",\"pig\",\"hbase\",\"kafka\",\"flink\",\"dask\",\"ray\",\n",
    "    # Orchestration & MLOps\n",
    "    \"airflow\",\"luigi\",\"mlflow\",\"prefect\",\"dagster\",\n",
    "    # Warehouses & db\n",
    "    \"snowflake\",\"bigquery\",\"redshift\",\"databricks\",\"athena\",\"glue\",\"synapse\",\"dbt\",\n",
    "    \"mysql\",\"postgresql\",\"postgres\",\"mssql\",\"oracle\",\"sqlite\",\"duckdb\",\"clickhouse\",\"elasticsearch\",\"neo4j\",\"mongodb\",\"cassandra\",\"dynamodb\",\n",
    "    # Cloud\n",
    "    \"aws\",\"azure\",\"gcp\",\"s3\",\"ec2\",\"emr\",\"lambda\",\"dataproc\",\n",
    "    # DevOps/containers\n",
    "    \"git\",\"github\",\"gitlab\",\"docker\",\"kubernetes\",\"terraform\",\n",
    "    # Business systems\n",
    "    \"sap\",\"salesforce\",\"netsuite\",\n",
    "    # Methods / themes\n",
    "    \"machine learning\",\"deep learning\",\"nlp\",\"natural language processing\",\"computer vision\",\n",
    "    \"statistics\",\"statistical inference\",\"hypothesis testing\",\"a/b testing\",\"experimentation\",\n",
    "    \"time series\",\"forecasting\",\"bayesian\",\"mcmc\",\n",
    "    \"optimization\",\"linear programming\",\"mixed integer programming\",\"mip\",\n",
    "    \"causal inference\",\"propensity score\",\"matching\",\n",
    "    \"feature engineering\",\"model interpretability\",\"shap\",\"lime\",\"calibration\",\"cross validation\",\n",
    "    \"dimensionality reduction\",\"pca\",\"cluster analysis\",\"k means\",\"k-means\",\n",
    "    \"data visualization\",\"data viz\",\"data mining\",\"data warehousing\",\"data warehouse\",\n",
    "    \"data modeling\",\"dimensional modeling\",\"star schema\",\"snowflake schema\",\n",
    "    \"data engineering\",\"data governance\",\"data quality\",\"data ethics\",\n",
    "    \"cloud computing\",\"cloud analytics\",\"business analytics\",\"marketing analytics\",\n",
    "    \"text analytics\",\"predictive analytics\",\"prescriptive analytics\",\n",
    "    \"big data\",\"database systems\",\"information systems\",\n",
    "    \"mlops\",\"model deployment\",\"model monitoring\",\n",
    "]\n",
    "ALIASES = {\n",
    "    \"scikit-learn\": [\"scikit learn\",\"sklearn\"],\n",
    "    \"power bi\": [\"powerbi\",\"power-bi\"],\n",
    "    \"postgresql\": [\"postgres\"],\n",
    "    \"a/b testing\": [\"ab testing\",\"a b testing\",\"a-b testing\"],\n",
    "    \"k-means\": [\"k means\"],\n",
    "    \"nlp\": [\"natural language processing\"],\n",
    "    \"mip\": [\"mixed integer programming\"],\n",
    "    \"data viz\": [\"data visualization\"],\n",
    "    \"data warehouse\": [\"data warehousing\"],\n",
    "    \"opencv\": [\"open cv\"],\n",
    "    \"xgboost\": [\"xgb\"],\n",
    "    \"mlops\": [\"ml ops\"],\n",
    "}\n",
    "\n",
    "def build_skill_vectorizer():\n",
    "    # Build variant -> canonical and vocabulary dict\n",
    "    variant_to_canon = {}\n",
    "    variants = []\n",
    "    for canon in RAW_SKILLS:\n",
    "        for v in [canon] + ALIASES.get(canon, []):\n",
    "            nv = norm_variant(v)\n",
    "            variants.append(nv)\n",
    "            # Use canonical normalized form as the target\n",
    "            variant_to_canon[nv] = norm_variant(canon)\n",
    "    # dedupe while preserving order\n",
    "    seen, vocab_list = set(), []\n",
    "    for v in variants:\n",
    "        if v not in seen and v.strip():\n",
    "            seen.add(v); vocab_list.append(v)\n",
    "    vocab_dict = {term: i for i, term in enumerate(vocab_list)}\n",
    "    vec = CountVectorizer(\n",
    "        vocabulary=vocab_dict,\n",
    "        ngram_range=(1,4),  # catch multiword skills and variants\n",
    "        token_pattern=r\"[A-Za-z0-9\\+\\#]+\"\n",
    "    )\n",
    "    idx_to_variant = {i: term for term, i in vocab_dict.items()}\n",
    "    return vec, idx_to_variant, variant_to_canon\n",
    "\n",
    "def skills_from_row(row_vector, idx_to_variant, variant_to_canon):\n",
    "    idxs = row_vector.indices\n",
    "    return sorted({ variant_to_canon[idx_to_variant[i]] for i in idxs })\n",
    "\n",
    "# ---------------- load data ----------------\n",
    "curr = pd.read_csv(CURR)\n",
    "jobs = pd.read_csv(JOBS)\n",
    "\n",
    "# curriculum text + university\n",
    "cur_text_cols = pick_cols(curr, [\"curriculum\",\"course\",\"syllabus\",\"program\",\"msba\",\"catalog\",\"description\"]) \\\n",
    "                or [c for c in curr.columns if curr[c].dtype == object][:1]\n",
    "curr[\"curriculum_text\"] = collapse_text(curr, cur_text_cols)\n",
    "\n",
    "uni_cands = pick_cols(curr, [\"university\",\"school\",\"college\",\"institution\"])\n",
    "if not uni_cands:\n",
    "    raise ValueError(\"No university column found in MSBA-top5.csv\")\n",
    "UNI_COL = uni_cands[0]\n",
    "curr[\"university\"] = (\n",
    "    curr[UNI_COL].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.title()\n",
    ")\n",
    "\n",
    "# job text + role\n",
    "job_text_cols = [c for c in jobs.columns if c.lower() in {\"final_job_description\",\"job_description\",\"description\"}] \\\n",
    "                or pick_cols(jobs, [\"job\",\"description\",\"posting\",\"jd\"]) \\\n",
    "                or [c for c in jobs.columns if jobs[c].dtype == object][:1]\n",
    "jobs[\"job_text\"] = collapse_text(jobs, job_text_cols)\n",
    "\n",
    "role_cands = pick_cols(jobs, [\"role\",\"title\",\"job_title\",\"position\"])\n",
    "ROLE_COL = role_cands[0] if role_cands else \"Role\"\n",
    "if ROLE_COL not in jobs.columns:\n",
    "    jobs[ROLE_COL] = \"Unknown Role\"\n",
    "\n",
    "# ---------------- extract skills ----------------\n",
    "skill_vec, idx_to_variant, variant_to_canon = build_skill_vectorizer()\n",
    "\n",
    "X_cur = skill_vec.transform(curr[\"curriculum_text\"].fillna(\"\").tolist())\n",
    "X_job = skill_vec.transform(jobs[\"job_text\"].fillna(\"\").tolist())\n",
    "\n",
    "curr[\"doc_skills\"] = [skills_from_row(X_cur.getrow(i), idx_to_variant, variant_to_canon)\n",
    "                      for i in range(X_cur.shape[0])]\n",
    "jobs[\"doc_skills\"] = [skills_from_row(X_job.getrow(i), idx_to_variant, variant_to_canon)\n",
    "                      for i in range(X_job.shape[0])]\n",
    "\n",
    "# ---------------- aggregate to exactly two outputs ----------------\n",
    "skills_from_curriculum = (\n",
    "    curr[[\"university\",\"doc_skills\"]]\n",
    "      .explode(\"doc_skills\")\n",
    "      .dropna(subset=[\"doc_skills\"])\n",
    "      .groupby([\"university\",\"doc_skills\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .rename(columns={\"doc_skills\":\"skill\"})\n",
    "      .sort_values([\"university\",\"count\"], ascending=[True,False])\n",
    ")\n",
    "\n",
    "skills_from_jobs = (\n",
    "    jobs[[ROLE_COL,\"doc_skills\"]]\n",
    "      .explode(\"doc_skills\")\n",
    "      .dropna(subset=[\"doc_skills\"])\n",
    "      .groupby([ROLE_COL,\"doc_skills\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .rename(columns={ROLE_COL:\"role\",\"doc_skills\":\"skill\"})\n",
    "      .sort_values([\"role\",\"count\"], ascending=[True,False])\n",
    ")\n",
    "\n",
    "skills_from_curriculum.to_csv(OUT/\"skills_from_curriculum.csv\", index=False)\n",
    "skills_from_jobs.to_csv(OUT/\"skills_from_jobs.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(OUT/\"skills_from_curriculum.csv\")\n",
    "print(OUT/\"skills_from_jobs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gZ2_SNwYGjX"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Top-5 skills-only n-grams per role (first column) + optional PPT slide.\n",
    "\n",
    "Inputs:\n",
    "  cleaned_job_data.csv  (first column = role bucket; a later column = job description)\n",
    "\n",
    "Outputs:\n",
    "  job_top5_skill_ngrams_by_first_column_roles.csv\n",
    "  Top_Skill_Signals_by_Role.pptx  (if MAKE_PPT=True)\n",
    "\n",
    "Edit the PATHS section if your file lives somewhere else.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ============ PATHS & SETTINGS ============\n",
    "INPUT_CSV = \"cleaned_job_data.csv\"   # change if needed\n",
    "OUT_DIR   = Path(\".\")                # change if needed\n",
    "TOP_K     = 5                        # top n-grams per role\n",
    "MAKE_PPT  = True                     # set False if you don't want the PPT\n",
    "# =========================================\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[-_/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def collapse_text(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).fillna(\"\")\n",
    "    return s.apply(lambda x: re.sub(r\"\\s+\", \" \", x.strip()))\n",
    "\n",
    "# A practical skills dictionary: tools, platforms, and methods\n",
    "SKILL_TERMS = [\n",
    "    # Languages & DB/query\n",
    "    \"python\",\"r\",\"java\",\"c\",\"c++\",\"c#\",\"scala\",\"julia\",\"matlab\",\"sql\",\"tsql\",\"nosql\",\n",
    "    \"mysql\",\"postgresql\",\"postgres\",\"mssql\",\"oracle\",\"sqlite\",\"duckdb\",\"clickhouse\",\n",
    "    # BI & viz\n",
    "    \"tableau\",\"power bi\",\"powerbi\",\"looker\",\"qlik\",\"excel\",\"vba\",\"metabase\",\"superset\",\n",
    "    \"data visualization\",\"dashboard\",\"dashboards\",\n",
    "    # Py data & viz libs\n",
    "    \"pandas\",\"numpy\",\"scipy\",\"statsmodels\",\"matplotlib\",\"plotly\",\"bokeh\",\"seaborn\",\n",
    "    # ML/DL libs & models\n",
    "    \"scikit-learn\",\"sklearn\",\"xgboost\",\"lightgbm\",\"catboost\",\"tensorflow\",\"pytorch\",\"keras\",\"fastai\",\n",
    "    # NLP/CV\n",
    "    \"spacy\",\"nltk\",\"transformers\",\"huggingface\",\"gensim\",\"word2vec\",\"bert\",\"roberta\",\"gpt\",\"opencv\",\n",
    "    # Data eng / big data\n",
    "    \"spark\",\"pyspark\",\"hadoop\",\"mapreduce\",\"hive\",\"kafka\",\"flink\",\"dask\",\"ray\",\"airflow\",\"luigi\",\"dbt\",\n",
    "    # Warehouses & cloud\n",
    "    \"snowflake\",\"bigquery\",\"redshift\",\"databricks\",\"athena\",\"glue\",\"synapse\",\n",
    "    \"aws\",\"azure\",\"gcp\",\"s3\",\"ec2\",\"emr\",\"lambda\",\"dataproc\",\n",
    "    # DevOps / MLOps\n",
    "    \"git\",\"github\",\"gitlab\",\"docker\",\"kubernetes\",\"terraform\",\"mlflow\",\"prefect\",\"dagster\",\n",
    "    # Methods / concepts\n",
    "    \"machine learning\",\"deep learning\",\"nlp\",\"natural language processing\",\"computer vision\",\n",
    "    \"statistics\",\"hypothesis testing\",\"a/b testing\",\"ab testing\",\"experimentation\",\n",
    "    \"causal inference\",\"propensity score\",\"matching\",\n",
    "    \"feature engineering\",\"model interpretability\",\"shap\",\"lime\",\"calibration\",\"cross validation\",\n",
    "    \"time series\",\"forecasting\",\"arima\",\"sarima\",\"bayesian\",\"mcmc\",\n",
    "    \"optimization\",\"linear programming\",\"mixed integer programming\",\"mip\",\n",
    "    \"regression\",\"classification\",\"clustering\",\"pca\",\"dimensionality reduction\",\n",
    "    # Data mgmt themes\n",
    "    \"etl\",\"elt\",\"data modeling\",\"dimensional modeling\",\"star schema\",\"snowflake schema\",\n",
    "    \"data warehousing\",\"data warehouse\",\"data governance\",\"data quality\",\"data engineering\",\n",
    "    \"cloud analytics\",\"business analytics\",\"text analytics\",\"predictive analytics\",\"prescriptive analytics\",\n",
    "]\n",
    "\n",
    "ALIASES = {\n",
    "    \"scikit-learn\": [\"scikit learn\",\"sklearn\"],\n",
    "    \"power bi\": [\"power-bi\",\"powerbi\"],\n",
    "    \"a/b testing\": [\"ab testing\",\"a b testing\",\"a-b testing\"],\n",
    "    \"postgresql\": [\"postgres\"],\n",
    "}\n",
    "\n",
    "def build_vocab_dict():\n",
    "    variants = []\n",
    "    for canon in SKILL_TERMS:\n",
    "        for v in [canon] + ALIASES.get(canon, []):\n",
    "            v = norm(v)\n",
    "            variants.append(v)\n",
    "            # add simple plural/ing variants for the last token\n",
    "            toks = v.split()\n",
    "            if toks:\n",
    "                last = toks[-1]\n",
    "                if not last.endswith(\"s\"):\n",
    "                    variants.append(norm(\" \".join(toks[:-1] + [last + \"s\"])))\n",
    "                if last.endswith(\"e\"):\n",
    "                    variants.append(norm(\" \".join(toks[:-1] + [last[:-1] + \"ing\"])))\n",
    "                elif not last.endswith(\"ing\") and last.isalpha():\n",
    "                    variants.append(norm(\" \".join(toks[:-1] + [last + \"ing\"])))\n",
    "    # dedupe preserving order\n",
    "    seen, vocab = set(), []\n",
    "    for t in variants:\n",
    "        if t and t not in seen:\n",
    "            seen.add(t); vocab.append(t)\n",
    "    return {term: i for i, term in enumerate(vocab)}\n",
    "\n",
    "def detect_text_col(df: pd.DataFrame, exclude_cols):\n",
    "    # try standard columns first\n",
    "    candidates = [c for c in df.columns if c.lower() in {\"final_job_description\",\"job_description\",\"description\"} and c not in exclude_cols]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    # fallback to first object-like column not excluded\n",
    "    for c in df.columns:\n",
    "        if c in exclude_cols:\n",
    "            continue\n",
    "        if df[c].dtype == object:\n",
    "            return c\n",
    "    # absolute last resort: next column\n",
    "    for c in df.columns:\n",
    "        if c not in exclude_cols:\n",
    "            return c\n",
    "\n",
    "def compute_topk_by_role(input_csv: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Need at least two columns: first = role bucket, plus a text column.\")\n",
    "\n",
    "    role_col = df.columns[0]  # first column as requested\n",
    "    text_col = detect_text_col(df, exclude_cols={role_col})\n",
    "\n",
    "    df[\"job_text\"] = collapse_text(df[text_col])\n",
    "\n",
    "    vocab_dict = build_vocab_dict()\n",
    "    vec = CountVectorizer(\n",
    "        vocabulary=vocab_dict,\n",
    "        ngram_range=(1,4),\n",
    "        token_pattern=r\"[A-Za-z0-9\\\\+\\\\#]+\"\n",
    "    )\n",
    "    X = vec.transform(df[\"job_text\"].tolist())\n",
    "\n",
    "    rows = []\n",
    "    # group by first column (role bucket)\n",
    "    for role, idx in df.groupby(role_col).indices.items():\n",
    "        sub = X[idx, :]\n",
    "        freqs = np.asarray(sub.sum(axis=0)).ravel()\n",
    "        terms = np.array(sorted(vocab_dict, key=lambda k: vocab_dict[k]))\n",
    "        order = np.argsort(-freqs)\n",
    "        top = [(terms[i], int(freqs[i])) for i in order if freqs[i] > 0][:top_k]\n",
    "        for rank, (ng, fr) in enumerate(top, start=1):\n",
    "            rows.append({\"role\": str(role), \"ngram\": ng, \"frequency\": fr, \"rank\": rank})\n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns=[\"role\",\"ngram\",\"frequency\",\"rank\"]).sort_values([\"role\",\"rank\"])\n",
    "    return out_df\n",
    "\n",
    "def save_ppt_from_table(df: pd.DataFrame, out_path: Path):\n",
    "    try:\n",
    "        # lazy import so people without python-pptx can still run CSV part\n",
    "        from pptx import Presentation\n",
    "        from pptx.util import Inches, Pt\n",
    "        from pptx.enum.shapes import MSO_SHAPE\n",
    "        from pptx.dml.color import RGBColor\n",
    "        from pptx.enum.text import PP_ALIGN\n",
    "    except Exception as e:\n",
    "        print(\"python-pptx not installed; skipping PPT. pip install python-pptx if you want it.\")\n",
    "        return\n",
    "\n",
    "    prs = Presentation()\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "\n",
    "    # Title\n",
    "    title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9.0), Inches(0.7))\n",
    "    tf = title_box.text_frame\n",
    "    tf.text = \"Top Skill Signals by Role (Jobs)\"\n",
    "    tf.paragraphs[0].font.size = Pt(28)\n",
    "    tf.paragraphs[0].font.bold = True\n",
    "\n",
    "    colors = {\n",
    "        \"data analyst\": RGBColor(0x25, 0x63, 0xEB),\n",
    "        \"business analyst\": RGBColor(0x08, 0x91, 0xB2),\n",
    "        \"data scientist\": RGBColor(0x16, 0xA3, 0x4A),\n",
    "        \"data engineer\": RGBColor(0x93, 0x33, 0xEA),\n",
    "        \"ml engineer\": RGBColor(0xDC, 0x26, 0x26),\n",
    "        \"other\": RGBColor(0x64, 0x74, 0x8B),\n",
    "    }\n",
    "\n",
    "    def add_card(left, top, role_name, rows5):\n",
    "        card_w, card_h = Inches(4.0), Inches(2.1)\n",
    "        card = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, left, top, card_w, card_h)\n",
    "        card.fill.solid(); card.fill.fore_color.rgb = RGBColor(0xFF,0xFF,0xFF)\n",
    "        card.line.color.rgb = RGBColor(0xEE,0xEE,0xEE)\n",
    "\n",
    "        pill = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, left+Inches(0.25), top+Inches(0.2), card_w-Inches(0.5), Inches(0.4))\n",
    "        pill.fill.solid(); pill.fill.fore_color.rgb = colors.get(role_name.lower(), RGBColor(0x44,0x44,0x44))\n",
    "        pill.line.fill.background()\n",
    "        pt = pill.text_frame; pt.clear()\n",
    "        p = pt.paragraphs[0]; p.text = role_name.upper(); p.font.color.rgb = RGBColor(0xFF,0xFF,0xFF); p.font.size = Pt(12); p.font.bold = True; p.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "        tb = slide.shapes.add_textbox(left+Inches(0.3), top+Inches(0.75), card_w-Inches(0.6), card_h-Inches(0.95))\n",
    "        ttf = tb.text_frame; ttf.word_wrap = True\n",
    "        for i, (skill, freq) in enumerate(rows5):\n",
    "            para = ttf.add_paragraph() if i else ttf.paragraphs[0]\n",
    "            para.text = f\"{skill} â€” {freq}\"\n",
    "            para.font.size = Pt(14)\n",
    "            if i == 0: para.font.bold = True\n",
    "\n",
    "    # choose three most common roles by row count or just the distinct found\n",
    "    roles = list(df[\"role\"].unique())\n",
    "    # layout: three cards across\n",
    "    lefts = [Inches(0.5), Inches(4.8), Inches(9.1)]\n",
    "    top = Inches(1.2)\n",
    "\n",
    "    for idx, role in enumerate(roles[:3]):  # first three roles in the file\n",
    "        top5 = df[df[\"role\"] == role].sort_values(\"rank\")[[\"ngram\",\"frequency\"]].values.tolist()\n",
    "        add_card(lefts[idx], top, role, top5)\n",
    "\n",
    "    prs.save(str(out_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out_df = compute_topk_by_role(INPUT_CSV, TOP_K)\n",
    "    csv_path = OUT_DIR / \"job_top5_skill_ngrams_by_first_column_roles.csv\"\n",
    "    out_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved CSV: {csv_path.resolve()}\")\n",
    "\n",
    "    if MAKE_PPT:\n",
    "        ppt_path = OUT_DIR / \"Top_Skill_Signals_by_Role.pptx\"\n",
    "        save_ppt_from_table(out_df, ppt_path)\n",
    "        print(f\"Saved PPT: {ppt_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efUZwRkZYGhd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0krGpW9-Fgi"
   },
   "source": [
    "# **TOP 5 US**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEHemUuoYGfZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Read the CSV files\n",
    "curriculum_df = pd.read_csv('skills_from_curriculum.csv')\n",
    "job_skills_df = pd.read_csv('step3_job_skills_by_role.csv')\n",
    "\n",
    "# Function to normalize skill names for better matching\n",
    "def normalize_skill(skill):\n",
    "    \"\"\"Normalize skill names by converting to lowercase and removing special characters\"\"\"\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "# Normalize skills in both dataframes\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# Define role categories with keyword matching patterns\n",
    "# Keywords are ordered by priority (more specific first)\n",
    "role_categories = {\n",
    "    'Data Scientist': ['data scientist', 'ds ', 'data science'],\n",
    "    'Data Analyst': ['data analyst', 'data analysis'],\n",
    "    'Business Analyst': ['business analyst', 'business intelligence analyst', 'bi analyst', 'business systems analyst']\n",
    "}\n",
    "\n",
    "# Create a category column for job roles using keyword matching\n",
    "def categorize_role(role):\n",
    "    \"\"\"\n",
    "    Categorize job role based on keyword matching.\n",
    "    Uses priority-based matching to handle overlapping keywords.\n",
    "    \"\"\"\n",
    "    role_lower = role.lower()\n",
    "\n",
    "    # Check each category's keywords\n",
    "    for category, keywords in role_categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in role_lower:\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "job_skills_df['role_category'] = job_skills_df['role'].apply(categorize_role)\n",
    "\n",
    "# Print categorization statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROLE CATEGORIZATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for category in role_categories.keys():\n",
    "    count = len(job_skills_df[job_skills_df['role_category'] == category])\n",
    "    unique_roles = job_skills_df[job_skills_df['role_category'] == category]['role'].nunique()\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  â€¢ Total records: {count}\")\n",
    "    print(f\"  â€¢ Unique role titles: {unique_roles}\")\n",
    "\n",
    "    # Show sample role titles\n",
    "    sample_roles = job_skills_df[job_skills_df['role_category'] == category]['role'].unique()[:5]\n",
    "    print(f\"  â€¢ Sample roles: {', '.join(sample_roles)}\")\n",
    "    print()\n",
    "\n",
    "uncategorized = len(job_skills_df[job_skills_df['role_category'].isna()])\n",
    "print(f\"Uncategorized roles: {uncategorized}\")\n",
    "print(f\"Total categorized: {len(job_skills_df[job_skills_df['role_category'].notna()])}\")\n",
    "print()\n",
    "\n",
    "# Plot 1: Role Categorization Distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart for categorization distribution\n",
    "category_counts = job_skills_df[job_skills_df['role_category'].notna()].groupby('role_category').size()\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "ax1.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "        startangle=90, colors=colors, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "ax1.set_title('Distribution of Job Roles by Category', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Bar chart for unique role titles per category\n",
    "unique_counts = []\n",
    "for category in role_categories.keys():\n",
    "    count = job_skills_df[job_skills_df['role_category'] == category]['role'].nunique()\n",
    "    unique_counts.append(count)\n",
    "\n",
    "ax2.bar(role_categories.keys(), unique_counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_title('Unique Job Titles per Category', fontsize=14, weight='bold', pad=20)\n",
    "ax2.set_ylabel('Number of Unique Titles', fontsize=11, weight='bold')\n",
    "ax2.set_xlabel('Role Category', fontsize=11, weight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(unique_counts):\n",
    "    ax2.text(i, v + 1, str(v), ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Role Categorization Distribution\\n\")\n",
    "\n",
    "# Filter to only keep categorized roles\n",
    "job_skills_df = job_skills_df[job_skills_df['role_category'].notna()]\n",
    "\n",
    "# Get all unique skills across both datasets\n",
    "all_skills = sorted(set(\n",
    "    list(curriculum_df['skill_normalized'].unique()) +\n",
    "    list(job_skills_df['skill_normalized'].unique())\n",
    "))\n",
    "\n",
    "# Create university skill vectors\n",
    "universities = curriculum_df['university'].unique()\n",
    "university_vectors = {}\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = curriculum_df[curriculum_df['university'] == uni]\n",
    "    skill_vector = {}\n",
    "    for skill in all_skills:\n",
    "        count = uni_skills[uni_skills['skill_normalized'] == skill]['count'].sum()\n",
    "        skill_vector[skill] = count\n",
    "    university_vectors[uni] = skill_vector\n",
    "\n",
    "# Create job role skill vectors (aggregated by category)\n",
    "role_vectors = {}\n",
    "\n",
    "for category in role_categories.keys():\n",
    "    category_skills = job_skills_df[job_skills_df['role_category'] == category]\n",
    "    skill_vector = {}\n",
    "    for skill in all_skills:\n",
    "        count = category_skills[category_skills['skill_normalized'] == skill]['count'].sum()\n",
    "        skill_vector[skill] = count\n",
    "    role_vectors[category] = skill_vector\n",
    "\n",
    "# Convert dictionaries to matrices for cosine similarity calculation\n",
    "def dict_to_vector(skill_dict, all_skills):\n",
    "    \"\"\"Convert skill dictionary to numpy array\"\"\"\n",
    "    return np.array([skill_dict.get(skill, 0) for skill in all_skills])\n",
    "\n",
    "# Calculate cosine similarity for each university-role combination\n",
    "results = []\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    role_vector = dict_to_vector(role_vectors[role], all_skills)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ALIGNMENT ANALYSIS FOR: {role}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    role_similarities = []\n",
    "\n",
    "    for uni in universities:\n",
    "        uni_vector = dict_to_vector(university_vectors[uni], all_skills)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            uni_vector.reshape(1, -1),\n",
    "            role_vector.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # Find matching skills (both have non-zero values)\n",
    "        matching_skills = [\n",
    "            skill for skill in all_skills\n",
    "            if university_vectors[uni][skill] > 0 and role_vectors[role][skill] > 0\n",
    "        ]\n",
    "\n",
    "        # Get top matching skills by job demand\n",
    "        top_skills = sorted(\n",
    "            matching_skills,\n",
    "            key=lambda s: role_vectors[role][s],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        role_similarities.append({\n",
    "            'University': uni,\n",
    "            'Role': role,\n",
    "            'Similarity_Score': similarity * 100,\n",
    "            'Matching_Skills_Count': len(matching_skills),\n",
    "            'Top_Matching_Skills': ', '.join(top_skills) if top_skills else 'None'\n",
    "        })\n",
    "\n",
    "    # Sort by similarity score\n",
    "    role_similarities = sorted(\n",
    "        role_similarities,\n",
    "        key=lambda x: x['Similarity_Score'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Print rankings\n",
    "    for rank, item in enumerate(role_similarities, 1):\n",
    "        medal = 'ðŸ¥‡' if rank == 1 else 'ðŸ¥ˆ' if rank == 2 else 'ðŸ¥‰' if rank == 3 else '  '\n",
    "        print(f\"{medal} Rank {rank}: {item['University']}\")\n",
    "        print(f\"   Similarity Score: {item['Similarity_Score']:.2f}%\")\n",
    "        print(f\"   Matching Skills: {item['Matching_Skills_Count']}\")\n",
    "        print(f\"   Top Skills: {item['Top_Matching_Skills']}\")\n",
    "        print()\n",
    "\n",
    "    results.extend(role_similarities)\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('university_job_alignment_results.csv', index=False)\n",
    "print(f\"\\nResults saved to 'university_job_alignment_results.csv'\")\n",
    "\n",
    "# Create summary pivot table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: Alignment Scores by University and Role\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='Similarity_Score',\n",
    "    index='University',\n",
    "    columns='Role',\n",
    "    aggfunc='first'\n",
    ").round(2)\n",
    "\n",
    "print(pivot_table)\n",
    "\n",
    "# Plot 2: Heatmap of Similarity Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Similarity Score (%)'},\n",
    "            linewidths=2, linecolor='white',\n",
    "            square=True, vmin=0, vmax=100)\n",
    "plt.title('University-Role Alignment Heatmap\\n(Cosine Similarity Scores)',\n",
    "          fontsize=14, weight='bold', pad=20)\n",
    "plt.xlabel('Role Category', fontsize=11, weight='bold')\n",
    "plt.ylabel('University', fontsize=11, weight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Similarity Heatmap\\n\")\n",
    "\n",
    "# Find best university for each role\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST UNIVERSITY FOR EACH ROLE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    best = results_df[results_df['Role'] == role].sort_values(\n",
    "        'Similarity_Score',\n",
    "        ascending=False\n",
    "    ).iloc[0]\n",
    "    print(f\"{role}:\")\n",
    "    print(f\"  ðŸ† {best['University']}\")\n",
    "    print(f\"  Score: {best['Similarity_Score']:.2f}%\")\n",
    "    print(f\"  Matching Skills: {best['Matching_Skills_Count']}\")\n",
    "    print()\n",
    "\n",
    "# Plot 3: Grouped Bar Chart - Similarity Scores by Role\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(universities))\n",
    "width = 0.25\n",
    "\n",
    "for i, role in enumerate(role_categories.keys()):\n",
    "    scores = [results_df[(results_df['University'] == uni) &\n",
    "                         (results_df['Role'] == role)]['Similarity_Score'].values[0]\n",
    "              for uni in universities]\n",
    "    ax.bar(x + i*width, scores, width, label=role, edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('University', fontsize=11, weight='bold')\n",
    "ax.set_ylabel('Similarity Score (%)', fontsize=11, weight='bold')\n",
    "ax.set_title('University-Role Alignment Comparison', fontsize=14, weight='bold', pad=20)\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels([uni.split('(')[0].strip() for uni in universities], rotation=45, ha='right')\n",
    "ax.legend(title='Role Category', fontsize=10, title_fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Similarity Comparison Bars\\n\")\n",
    "\n",
    "# Plot 4: Best University for Each Role (Visual Summary)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "colors_role = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, (role, color) in enumerate(zip(role_categories.keys(), colors_role)):\n",
    "    role_data = results_df[results_df['Role'] == role].sort_values('Similarity_Score', ascending=True)\n",
    "\n",
    "    axes[idx].barh(range(len(role_data)), role_data['Similarity_Score'],\n",
    "                   color=color, edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_yticks(range(len(role_data)))\n",
    "    axes[idx].set_yticklabels([uni.split('(')[0].strip() for uni in role_data['University']], fontsize=9)\n",
    "    axes[idx].set_xlabel('Similarity Score (%)', fontsize=10, weight='bold')\n",
    "    axes[idx].set_title(f'{role}', fontsize=12, weight='bold', pad=10)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(role_data['Similarity_Score']):\n",
    "        axes[idx].text(v + 0.5, i, f'{v:.1f}%', va='center', fontsize=9, weight='bold')\n",
    "\n",
    "plt.suptitle('University Rankings by Role Category', fontsize=16, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Rankings by Role\\n\")\n",
    "\n",
    "# Create a detailed skills comparison\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED SKILLS ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    print(f\"\\n{role} - Top 10 Required Skills:\")\n",
    "    role_skills = sorted(\n",
    "        [(skill, role_vectors[role][skill]) for skill in all_skills if role_vectors[role][skill] > 0],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "\n",
    "    for skill, count in role_skills:\n",
    "        print(f\"  â€¢ {skill}: {count} job postings\")\n",
    "\n",
    "    # Show which universities teach these top skills\n",
    "    print(f\"\\n  Universities teaching these skills:\")\n",
    "    for uni in universities:\n",
    "        taught_count = sum(1 for skill, _ in role_skills if university_vectors[uni][skill] > 0)\n",
    "        coverage = (taught_count / len(role_skills)) * 100\n",
    "        print(f\"    {uni}: {taught_count}/{len(role_skills)} skills ({coverage:.1f}% coverage)\")\n",
    "    print()\n",
    "\n",
    "# Plot 5: Skills Coverage Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, role in enumerate(role_categories.keys()):\n",
    "    # Get top 10 skills for this role\n",
    "    role_skills = sorted(\n",
    "        [(skill, role_vectors[role][skill]) for skill in all_skills if role_vectors[role][skill] > 0],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "\n",
    "    # Calculate coverage for each university\n",
    "    coverage_data = []\n",
    "    for uni in universities:\n",
    "        taught_count = sum(1 for skill, _ in role_skills if university_vectors[uni][skill] > 0)\n",
    "        coverage_pct = (taught_count / len(role_skills)) * 100\n",
    "        coverage_data.append(coverage_pct)\n",
    "\n",
    "    axes[idx].bar(range(len(universities)), coverage_data,\n",
    "                  color=colors_role[idx], edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_xticks(range(len(universities)))\n",
    "    axes[idx].set_xticklabels([uni.split('(')[0].strip() for uni in universities],\n",
    "                               rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Coverage (%)', fontsize=10, weight='bold')\n",
    "    axes[idx].set_title(f'{role}\\nTop 10 Skills Coverage', fontsize=11, weight='bold')\n",
    "    axes[idx].set_ylim(0, 100)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(coverage_data):\n",
    "        axes[idx].text(i, v + 2, f'{v:.0f}%', ha='center', va='bottom', fontsize=9, weight='bold')\n",
    "\n",
    "plt.suptitle('University Coverage of Top 10 In-Demand Skills by Role',\n",
    "             fontsize=14, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Skills Coverage Analysis\\n\")\n",
    "\n",
    "# Plot 6: Matching Skills Count Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "matching_pivot = results_df.pivot_table(\n",
    "    values='Matching_Skills_Count',\n",
    "    index='University',\n",
    "    columns='Role',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "matching_pivot.plot(kind='bar', ax=ax, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('University', fontsize=11, weight='bold')\n",
    "ax.set_ylabel('Number of Matching Skills', fontsize=11, weight='bold')\n",
    "ax.set_title('Number of Matching Skills Between Curriculum and Job Requirements',\n",
    "             fontsize=14, weight='bold', pad=20)\n",
    "ax.set_xticklabels([uni.split('(')[0].strip() for uni in matching_pivot.index],\n",
    "                    rotation=45, ha='right')\n",
    "ax.legend(title='Role Category', fontsize=10, title_fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Matching Skills Comparison\\n\")\n",
    "\n",
    "# Plot 7: Radar Chart for Overall Alignment\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = [n / float(len(role_categories)) * 2 * pi for n in range(len(role_categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "for uni in universities:\n",
    "    values = []\n",
    "    for role in role_categories.keys():\n",
    "        score = results_df[(results_df['University'] == uni) &\n",
    "                          (results_df['Role'] == role)]['Similarity_Score'].values[0]\n",
    "        values.append(score)\n",
    "    values += values[:1]\n",
    "\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=uni.split('(')[0].strip())\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(role_categories.keys(), fontsize=11, weight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel('Similarity Score (%)', fontsize=11, weight='bold', labelpad=30)\n",
    "ax.set_title('Overall University-Role Alignment Profile',\n",
    "             fontsize=14, weight='bold', pad=30)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Alignment Radar Chart\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL VISUALIZATIONS DISPLAYED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š Displayed plots:\")\n",
    "print(\"  1. Role Categorization Distribution - Role distribution and unique titles\")\n",
    "print(\"  2. Similarity Heatmap - Heatmap of all similarity scores\")\n",
    "print(\"  3. Similarity Comparison Bars - Grouped bar chart comparison\")\n",
    "print(\"  4. Rankings by Role - Horizontal bar charts showing rankings\")\n",
    "print(\"  5. Skills Coverage Analysis - Top 10 skills coverage by university\")\n",
    "print(\"  6. Matching Skills Comparison - Number of matching skills\")\n",
    "print(\"  7. Alignment Radar Chart - Overall alignment profile radar chart\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-enC2kBI-U9c"
   },
   "outputs": [],
   "source": [
    "# --- Curriculum vs Job Skill Alignment for All Roles ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load data ---\n",
    "curriculum_df = pd.read_csv('skills_from_curriculum.csv')\n",
    "job_skills_df = pd.read_csv('step3_job_skills_by_role.csv')\n",
    "\n",
    "# --- 1ï¸âƒ£ Normalize skill names ---\n",
    "def normalize_skill(skill):\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9\\s]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# --- 2ï¸âƒ£ Function to get filtered job skills for a role ---\n",
    "def get_role_skills(df, role_name):\n",
    "    role_name_lower = role_name.lower()\n",
    "    if 'role_category' in df.columns:\n",
    "        role_df = df[df['role_category'].str.lower() == role_name_lower]\n",
    "    else:\n",
    "        role_df = df[df['role'].str.lower().str.contains(role_name_lower, na=False)]\n",
    "\n",
    "    skills = role_df['skill_normalized'].dropna().unique().tolist()\n",
    "    return list(set(skills))  # remove duplicates\n",
    "\n",
    "# --- 3ï¸âƒ£ Extract skills for each role ---\n",
    "data_scientist_skills = get_role_skills(job_skills_df, 'Data Scientist')\n",
    "data_analyst_skills = get_role_skills(job_skills_df, 'Data Analyst')\n",
    "business_analyst_skills = get_role_skills(job_skills_df, 'Business Analyst')\n",
    "\n",
    "print(f\"âœ… Data Scientist skills collected: {len(data_scientist_skills)}\")\n",
    "print(f\"âœ… Data Analyst skills collected: {len(data_analyst_skills)}\")\n",
    "print(f\"âœ… Business Analyst skills collected: {len(business_analyst_skills)}\")\n",
    "\n",
    "# --- 4ï¸âƒ£ Create combined vocabulary ---\n",
    "all_skills = sorted(set(\n",
    "    list(curriculum_df['skill_normalized'].unique()) +\n",
    "    data_scientist_skills + data_analyst_skills + business_analyst_skills\n",
    "))\n",
    "\n",
    "# --- 5ï¸âƒ£ Build curriculum vectors for universities ---\n",
    "universities = curriculum_df['university'].unique()\n",
    "university_vectors = {}\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = curriculum_df[curriculum_df['university'] == uni]['skill_normalized'].tolist()\n",
    "    uni_vector = np.array([1 if skill in uni_skills else 0 for skill in all_skills])\n",
    "    university_vectors[uni] = uni_vector\n",
    "\n",
    "# --- 6ï¸âƒ£ Function to compute similarity for a given role ---\n",
    "def compute_similarity(role_skills, role_name):\n",
    "    role_vector = np.array([1 if skill in role_skills else 0 for skill in all_skills])\n",
    "\n",
    "    scores = {}\n",
    "    for uni, uni_vector in university_vectors.items():\n",
    "        sim = cosine_similarity([uni_vector], [role_vector])[0][0]\n",
    "        scores[uni] = sim\n",
    "\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['University', f'Similarity_with_{role_name.replace(\" \", \"_\")}'])\n",
    "    df = df.sort_values(by=df.columns[1], ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_name = f\"{role_name.replace(' ', '_')}_Curriculum_Alignment.csv\"\n",
    "    df.to_csv(csv_name, index=False)\n",
    "\n",
    "    print(f\"\\n=== Curriculum vs {role_name} Skill Alignment ===\")\n",
    "    print(df)\n",
    "    print(f\"âœ… Saved as '{csv_name}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- 7ï¸âƒ£ Run for all three roles ---\n",
    "df_ds = compute_similarity(data_scientist_skills, 'Data Scientist')\n",
    "df_da = compute_similarity(data_analyst_skills, 'Data Analyst')\n",
    "df_ba = compute_similarity(business_analyst_skills, 'Business Analyst')\n",
    "\n",
    "# --- 8ï¸âƒ£ Combine for summary ---\n",
    "summary_df = df_ds.merge(df_da, on='University').merge(df_ba, on='University')\n",
    "summary_df.to_csv(\"AllRoles_Curriculum_Alignment.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Combined alignment saved as 'AllRoles_Curriculum_Alignment.csv'\")\n",
    "\n",
    "# --- 9ï¸âƒ£ Optional Visualization ---\n",
    "summary_df.plot(\n",
    "    x='University',\n",
    "    kind='bar',\n",
    "    figsize=(12,6),\n",
    "    title='Curriculum Alignment with Job Roles',\n",
    "    color=['#40be95ff', '#0072B2', '#D55E00']\n",
    ")\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Data Scientist', 'Data Analyst', 'Business Analyst'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('top5us')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYkV8WgH95Tb"
   },
   "source": [
    "# **NON- US**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2JGW8BNYGcy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Read the CSV files\n",
    "curriculum_df = pd.read_csv('skills_from_curriculum_non_us.csv')\n",
    "job_skills_df = pd.read_csv('step3_job_skills_by_role.csv')\n",
    "\n",
    "# Function to normalize skill names for better matching\n",
    "def normalize_skill(skill):\n",
    "    \"\"\"Normalize skill names by converting to lowercase and removing special characters\"\"\"\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "# Normalize skills in both dataframes\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# Define role categories with keyword matching patterns\n",
    "# Keywords are ordered by priority (more specific first)\n",
    "role_categories = {\n",
    "    'Data Scientist': ['data scientist', 'ds ', 'data science'],\n",
    "    'Data Analyst': ['data analyst', 'data analysis'],\n",
    "    'Business Analyst': ['business analyst', 'business intelligence analyst', 'bi analyst', 'business systems analyst']\n",
    "}\n",
    "\n",
    "# Create a category column for job roles using keyword matching\n",
    "def categorize_role(role):\n",
    "    \"\"\"\n",
    "    Categorize job role based on keyword matching.\n",
    "    Uses priority-based matching to handle overlapping keywords.\n",
    "    \"\"\"\n",
    "    role_lower = role.lower()\n",
    "\n",
    "    # Check each category's keywords\n",
    "    for category, keywords in role_categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in role_lower:\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "job_skills_df['role_category'] = job_skills_df['role'].apply(categorize_role)\n",
    "\n",
    "# Print categorization statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROLE CATEGORIZATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for category in role_categories.keys():\n",
    "    count = len(job_skills_df[job_skills_df['role_category'] == category])\n",
    "    unique_roles = job_skills_df[job_skills_df['role_category'] == category]['role'].nunique()\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  â€¢ Total records: {count}\")\n",
    "    print(f\"  â€¢ Unique role titles: {unique_roles}\")\n",
    "\n",
    "    # Show sample role titles\n",
    "    sample_roles = job_skills_df[job_skills_df['role_category'] == category]['role'].unique()[:5]\n",
    "    print(f\"  â€¢ Sample roles: {', '.join(sample_roles)}\")\n",
    "    print()\n",
    "\n",
    "uncategorized = len(job_skills_df[job_skills_df['role_category'].isna()])\n",
    "print(f\"Uncategorized roles: {uncategorized}\")\n",
    "print(f\"Total categorized: {len(job_skills_df[job_skills_df['role_category'].notna()])}\")\n",
    "print()\n",
    "\n",
    "# Plot 1: Role Categorization Distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart for categorization distribution\n",
    "category_counts = job_skills_df[job_skills_df['role_category'].notna()].groupby('role_category').size()\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "ax1.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "        startangle=90, colors=colors, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "ax1.set_title('Distribution of Job Roles by Category', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Bar chart for unique role titles per category\n",
    "unique_counts = []\n",
    "for category in role_categories.keys():\n",
    "    count = job_skills_df[job_skills_df['role_category'] == category]['role'].nunique()\n",
    "    unique_counts.append(count)\n",
    "\n",
    "ax2.bar(role_categories.keys(), unique_counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_title('Unique Job Titles per Category', fontsize=14, weight='bold', pad=20)\n",
    "ax2.set_ylabel('Number of Unique Titles', fontsize=11, weight='bold')\n",
    "ax2.set_xlabel('Role Category', fontsize=11, weight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(unique_counts):\n",
    "    ax2.text(i, v + 1, str(v), ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Role Categorization Distribution\\n\")\n",
    "\n",
    "# Filter to only keep categorized roles\n",
    "job_skills_df = job_skills_df[job_skills_df['role_category'].notna()]\n",
    "\n",
    "# Get all unique skills across both datasets\n",
    "all_skills = sorted(set(\n",
    "    list(curriculum_df['skill_normalized'].unique()) +\n",
    "    list(job_skills_df['skill_normalized'].unique())\n",
    "))\n",
    "\n",
    "# Create university skill vectors\n",
    "universities = curriculum_df['university'].unique()\n",
    "university_vectors = {}\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = curriculum_df[curriculum_df['university'] == uni]\n",
    "    skill_vector = {}\n",
    "    for skill in all_skills:\n",
    "        count = uni_skills[uni_skills['skill_normalized'] == skill]['count'].sum()\n",
    "        skill_vector[skill] = count\n",
    "    university_vectors[uni] = skill_vector\n",
    "\n",
    "# Create job role skill vectors (aggregated by category)\n",
    "role_vectors = {}\n",
    "\n",
    "for category in role_categories.keys():\n",
    "    category_skills = job_skills_df[job_skills_df['role_category'] == category]\n",
    "    skill_vector = {}\n",
    "    for skill in all_skills:\n",
    "        count = category_skills[category_skills['skill_normalized'] == skill]['count'].sum()\n",
    "        skill_vector[skill] = count\n",
    "    role_vectors[category] = skill_vector\n",
    "\n",
    "# Convert dictionaries to matrices for cosine similarity calculation\n",
    "def dict_to_vector(skill_dict, all_skills):\n",
    "    \"\"\"Convert skill dictionary to numpy array\"\"\"\n",
    "    return np.array([skill_dict.get(skill, 0) for skill in all_skills])\n",
    "\n",
    "# Calculate cosine similarity for each university-role combination\n",
    "results = []\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    role_vector = dict_to_vector(role_vectors[role], all_skills)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ALIGNMENT ANALYSIS FOR: {role}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    role_similarities = []\n",
    "\n",
    "    for uni in universities:\n",
    "        uni_vector = dict_to_vector(university_vectors[uni], all_skills)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            uni_vector.reshape(1, -1),\n",
    "            role_vector.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # Find matching skills (both have non-zero values)\n",
    "        matching_skills = [\n",
    "            skill for skill in all_skills\n",
    "            if university_vectors[uni][skill] > 0 and role_vectors[role][skill] > 0\n",
    "        ]\n",
    "\n",
    "        # Get top matching skills by job demand\n",
    "        top_skills = sorted(\n",
    "            matching_skills,\n",
    "            key=lambda s: role_vectors[role][s],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        role_similarities.append({\n",
    "            'University': uni,\n",
    "            'Role': role,\n",
    "            'Similarity_Score': similarity * 100,\n",
    "            'Matching_Skills_Count': len(matching_skills),\n",
    "            'Top_Matching_Skills': ', '.join(top_skills) if top_skills else 'None'\n",
    "        })\n",
    "\n",
    "    # Sort by similarity score\n",
    "    role_similarities = sorted(\n",
    "        role_similarities,\n",
    "        key=lambda x: x['Similarity_Score'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Print rankings\n",
    "    for rank, item in enumerate(role_similarities, 1):\n",
    "        medal = 'ðŸ¥‡' if rank == 1 else 'ðŸ¥ˆ' if rank == 2 else 'ðŸ¥‰' if rank == 3 else '  '\n",
    "        print(f\"{medal} Rank {rank}: {item['University']}\")\n",
    "        print(f\"   Similarity Score: {item['Similarity_Score']:.2f}%\")\n",
    "        print(f\"   Matching Skills: {item['Matching_Skills_Count']}\")\n",
    "        print(f\"   Top Skills: {item['Top_Matching_Skills']}\")\n",
    "        print()\n",
    "\n",
    "    results.extend(role_similarities)\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('university_job_alignment_results.csv', index=False)\n",
    "print(f\"\\nResults saved to 'university_job_alignment_results.csv'\")\n",
    "\n",
    "# Create summary pivot table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: Alignment Scores by University and Role\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='Similarity_Score',\n",
    "    index='University',\n",
    "    columns='Role',\n",
    "    aggfunc='first'\n",
    ").round(2)\n",
    "\n",
    "print(pivot_table)\n",
    "\n",
    "# Plot 2: Heatmap of Similarity Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Similarity Score (%)'},\n",
    "            linewidths=2, linecolor='white',\n",
    "            square=True, vmin=0, vmax=100)\n",
    "plt.title('University-Role Alignment Heatmap\\n(Cosine Similarity Scores)',\n",
    "          fontsize=14, weight='bold', pad=20)\n",
    "plt.xlabel('Role Category', fontsize=11, weight='bold')\n",
    "plt.ylabel('University', fontsize=11, weight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Similarity Heatmap\\n\")\n",
    "\n",
    "# Find best university for each role\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST UNIVERSITY FOR EACH ROLE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    best = results_df[results_df['Role'] == role].sort_values(\n",
    "        'Similarity_Score',\n",
    "        ascending=False\n",
    "    ).iloc[0]\n",
    "    print(f\"{role}:\")\n",
    "    print(f\"  ðŸ† {best['University']}\")\n",
    "    print(f\"  Score: {best['Similarity_Score']:.2f}%\")\n",
    "    print(f\"  Matching Skills: {best['Matching_Skills_Count']}\")\n",
    "    print()\n",
    "\n",
    "# Plot 3: Grouped Bar Chart - Similarity Scores by Role\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(universities))\n",
    "width = 0.25\n",
    "\n",
    "for i, role in enumerate(role_categories.keys()):\n",
    "    scores = [results_df[(results_df['University'] == uni) &\n",
    "                         (results_df['Role'] == role)]['Similarity_Score'].values[0]\n",
    "              for uni in universities]\n",
    "    ax.bar(x + i*width, scores, width, label=role, edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('University', fontsize=11, weight='bold')\n",
    "ax.set_ylabel('Similarity Score (%)', fontsize=11, weight='bold')\n",
    "ax.set_title('University-Role Alignment Comparison', fontsize=14, weight='bold', pad=20)\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels([uni.split('(')[0].strip() for uni in universities], rotation=45, ha='right')\n",
    "ax.legend(title='Role Category', fontsize=10, title_fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Similarity Comparison Bars\\n\")\n",
    "\n",
    "# Plot 4: Best University for Each Role (Visual Summary)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "colors_role = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, (role, color) in enumerate(zip(role_categories.keys(), colors_role)):\n",
    "    role_data = results_df[results_df['Role'] == role].sort_values('Similarity_Score', ascending=True)\n",
    "\n",
    "    axes[idx].barh(range(len(role_data)), role_data['Similarity_Score'],\n",
    "                   color=color, edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_yticks(range(len(role_data)))\n",
    "    axes[idx].set_yticklabels([uni.split('(')[0].strip() for uni in role_data['University']], fontsize=9)\n",
    "    axes[idx].set_xlabel('Similarity Score (%)', fontsize=10, weight='bold')\n",
    "    axes[idx].set_title(f'{role}', fontsize=12, weight='bold', pad=10)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(role_data['Similarity_Score']):\n",
    "        axes[idx].text(v + 0.5, i, f'{v:.1f}%', va='center', fontsize=9, weight='bold')\n",
    "\n",
    "plt.suptitle('University Rankings by Role Category', fontsize=16, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Rankings by Role\\n\")\n",
    "\n",
    "# Create a detailed skills comparison\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED SKILLS ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for role in role_categories.keys():\n",
    "    print(f\"\\n{role} - Top 10 Required Skills:\")\n",
    "    role_skills = sorted(\n",
    "        [(skill, role_vectors[role][skill]) for skill in all_skills if role_vectors[role][skill] > 0],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "\n",
    "    for skill, count in role_skills:\n",
    "        print(f\"  â€¢ {skill}: {count} job postings\")\n",
    "\n",
    "    # Show which universities teach these top skills\n",
    "    print(f\"\\n  Universities teaching these skills:\")\n",
    "    for uni in universities:\n",
    "        taught_count = sum(1 for skill, _ in role_skills if university_vectors[uni][skill] > 0)\n",
    "        coverage = (taught_count / len(role_skills)) * 100\n",
    "        print(f\"    {uni}: {taught_count}/{len(role_skills)} skills ({coverage:.1f}% coverage)\")\n",
    "    print()\n",
    "\n",
    "# Plot 5: Skills Coverage Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, role in enumerate(role_categories.keys()):\n",
    "    # Get top 10 skills for this role\n",
    "    role_skills = sorted(\n",
    "        [(skill, role_vectors[role][skill]) for skill in all_skills if role_vectors[role][skill] > 0],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "\n",
    "    # Calculate coverage for each university\n",
    "    coverage_data = []\n",
    "    for uni in universities:\n",
    "        taught_count = sum(1 for skill, _ in role_skills if university_vectors[uni][skill] > 0)\n",
    "        coverage_pct = (taught_count / len(role_skills)) * 100\n",
    "        coverage_data.append(coverage_pct)\n",
    "\n",
    "    axes[idx].bar(range(len(universities)), coverage_data,\n",
    "                  color=colors_role[idx], edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_xticks(range(len(universities)))\n",
    "    axes[idx].set_xticklabels([uni.split('(')[0].strip() for uni in universities],\n",
    "                               rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Coverage (%)', fontsize=10, weight='bold')\n",
    "    axes[idx].set_title(f'{role}\\nTop 10 Skills Coverage', fontsize=11, weight='bold')\n",
    "    axes[idx].set_ylim(0, 100)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(coverage_data):\n",
    "        axes[idx].text(i, v + 2, f'{v:.0f}%', ha='center', va='bottom', fontsize=9, weight='bold')\n",
    "\n",
    "plt.suptitle('University Coverage of Top 10 In-Demand Skills by Role',\n",
    "             fontsize=14, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Skills Coverage Analysis\\n\")\n",
    "\n",
    "# Plot 6: Matching Skills Count Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "matching_pivot = results_df.pivot_table(\n",
    "    values='Matching_Skills_Count',\n",
    "    index='University',\n",
    "    columns='Role',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "matching_pivot.plot(kind='bar', ax=ax, edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('University', fontsize=11, weight='bold')\n",
    "ax.set_ylabel('Number of Matching Skills', fontsize=11, weight='bold')\n",
    "ax.set_title('Number of Matching Skills Between Curriculum and Job Requirements',\n",
    "             fontsize=14, weight='bold', pad=20)\n",
    "ax.set_xticklabels([uni.split('(')[0].strip() for uni in matching_pivot.index],\n",
    "                    rotation=45, ha='right')\n",
    "ax.legend(title='Role Category', fontsize=10, title_fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Matching Skills Comparison\\n\")\n",
    "\n",
    "# Plot 7: Radar Chart for Overall Alignment\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = [n / float(len(role_categories)) * 2 * pi for n in range(len(role_categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "for uni in universities:\n",
    "    values = []\n",
    "    for role in role_categories.keys():\n",
    "        score = results_df[(results_df['University'] == uni) &\n",
    "                          (results_df['Role'] == role)]['Similarity_Score'].values[0]\n",
    "        values.append(score)\n",
    "    values += values[:1]\n",
    "\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=uni.split('(')[0].strip())\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(role_categories.keys(), fontsize=11, weight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel('Similarity Score (%)', fontsize=11, weight='bold', labelpad=30)\n",
    "ax.set_title('Overall University-Role Alignment Profile',\n",
    "             fontsize=14, weight='bold', pad=30)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ“Š Plot displayed: Alignment Radar Chart\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL VISUALIZATIONS DISPLAYED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š Displayed plots:\")\n",
    "print(\"  1. Role Categorization Distribution - Role distribution and unique titles\")\n",
    "print(\"  2. Similarity Heatmap - Heatmap of all similarity scores\")\n",
    "print(\"  3. Similarity Comparison Bars - Grouped bar chart comparison\")\n",
    "print(\"  4. Rankings by Role - Horizontal bar charts showing rankings\")\n",
    "print(\"  5. Skills Coverage Analysis - Top 10 skills coverage by university\")\n",
    "print(\"  6. Matching Skills Comparison - Number of matching skills\")\n",
    "print(\"  7. Alignment Radar Chart - Overall alignment profile radar chart\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsNk6lDMYGax"
   },
   "outputs": [],
   "source": [
    "# --- Curriculum vs Job Skill Alignment for All Roles ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load data ---\n",
    "curriculum_df = pd.read_csv('skills_from_curriculum_non_us.csv')\n",
    "job_skills_df = pd.read_csv('step3_job_skills_by_role.csv')\n",
    "\n",
    "# --- 1ï¸âƒ£ Normalize skill names ---\n",
    "def normalize_skill(skill):\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9\\s]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# --- 2ï¸âƒ£ Function to get filtered job skills for a role ---\n",
    "def get_role_skills(df, role_name):\n",
    "    role_name_lower = role_name.lower()\n",
    "    if 'role_category' in df.columns:\n",
    "        role_df = df[df['role_category'].str.lower() == role_name_lower]\n",
    "    else:\n",
    "        role_df = df[df['role'].str.lower().str.contains(role_name_lower, na=False)]\n",
    "\n",
    "    skills = role_df['skill_normalized'].dropna().unique().tolist()\n",
    "    return list(set(skills))  # remove duplicates\n",
    "\n",
    "# --- 3ï¸âƒ£ Extract skills for each role ---\n",
    "data_scientist_skills = get_role_skills(job_skills_df, 'Data Scientist')\n",
    "data_analyst_skills = get_role_skills(job_skills_df, 'Data Analyst')\n",
    "business_analyst_skills = get_role_skills(job_skills_df, 'Business Analyst')\n",
    "\n",
    "print(f\"âœ… Data Scientist skills collected: {len(data_scientist_skills)}\")\n",
    "print(f\"âœ… Data Analyst skills collected: {len(data_analyst_skills)}\")\n",
    "print(f\"âœ… Business Analyst skills collected: {len(business_analyst_skills)}\")\n",
    "\n",
    "# --- 4ï¸âƒ£ Create combined vocabulary ---\n",
    "all_skills = sorted(set(\n",
    "    list(curriculum_df['skill_normalized'].unique()) +\n",
    "    data_scientist_skills + data_analyst_skills + business_analyst_skills\n",
    "))\n",
    "\n",
    "# --- 5ï¸âƒ£ Build curriculum vectors for universities ---\n",
    "universities = curriculum_df['university'].unique()\n",
    "university_vectors = {}\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = curriculum_df[curriculum_df['university'] == uni]['skill_normalized'].tolist()\n",
    "    uni_vector = np.array([1 if skill in uni_skills else 0 for skill in all_skills])\n",
    "    university_vectors[uni] = uni_vector\n",
    "\n",
    "# --- 6ï¸âƒ£ Function to compute similarity for a given role ---\n",
    "def compute_similarity(role_skills, role_name):\n",
    "    role_vector = np.array([1 if skill in role_skills else 0 for skill in all_skills])\n",
    "\n",
    "    scores = {}\n",
    "    for uni, uni_vector in university_vectors.items():\n",
    "        sim = cosine_similarity([uni_vector], [role_vector])[0][0]\n",
    "        scores[uni] = sim\n",
    "\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['University', f'Similarity_with_{role_name.replace(\" \", \"_\")}'])\n",
    "    df = df.sort_values(by=df.columns[1], ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_name = f\"{role_name.replace(' ', '_')}_Curriculum_Alignment.csv\"\n",
    "    df.to_csv(csv_name, index=False)\n",
    "\n",
    "    print(f\"\\n=== Curriculum vs {role_name} Skill Alignment ===\")\n",
    "    print(df)\n",
    "    print(f\"âœ… Saved as '{csv_name}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- 7ï¸âƒ£ Run for all three roles ---\n",
    "df_ds = compute_similarity(data_scientist_skills, 'Data Scientist')\n",
    "df_da = compute_similarity(data_analyst_skills, 'Data Analyst')\n",
    "df_ba = compute_similarity(business_analyst_skills, 'Business Analyst')\n",
    "\n",
    "# --- 8ï¸âƒ£ Combine for summary ---\n",
    "summary_df = df_ds.merge(df_da, on='University').merge(df_ba, on='University')\n",
    "summary_df.to_csv(\"AllRoles_Curriculum_Alignment.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Combined alignment saved as 'AllRoles_Curriculum_Alignment.csv'\")\n",
    "\n",
    "# --- 9ï¸âƒ£ Optional Visualization ---\n",
    "summary_df.plot(\n",
    "    x='University',\n",
    "    kind='bar',\n",
    "    figsize=(12,6),\n",
    "    title='Curriculum Alignment with Job Roles',\n",
    "    color=['#2ea597', '#688ae8', '#8456ce']\n",
    ")\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Data Scientist', 'Data Analyst', 'Business Analyst'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('top5nonus.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jWW_Au7YGK4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY_HnlGJ-jm2"
   },
   "source": [
    "# **TOP 10 US**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emKrG2Ub-oiV"
   },
   "outputs": [],
   "source": [
    "# --- Curriculum vs Job Skill Alignment for All Roles ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load data ---\n",
    "curriculum_df = pd.read_csv('skills_from_curriculum (1).csv')\n",
    "job_skills_df = pd.read_csv('step3_job_skills_by_role.csv')\n",
    "\n",
    "# --- 1ï¸âƒ£ Normalize skill names ---\n",
    "def normalize_skill(skill):\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9\\s]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# --- 2ï¸âƒ£ Function to get filtered job skills for a role ---\n",
    "def get_role_skills(df, role_name):\n",
    "    role_name_lower = role_name.lower()\n",
    "    if 'role_category' in df.columns:\n",
    "        role_df = df[df['role_category'].str.lower() == role_name_lower]\n",
    "    else:\n",
    "        role_df = df[df['role'].str.lower().str.contains(role_name_lower, na=False)]\n",
    "\n",
    "    skills = role_df['skill_normalized'].dropna().unique().tolist()\n",
    "    return list(set(skills))  # remove duplicates\n",
    "\n",
    "# --- 3ï¸âƒ£ Extract skills for each role ---\n",
    "data_scientist_skills = get_role_skills(job_skills_df, 'Data Scientist')\n",
    "data_analyst_skills = get_role_skills(job_skills_df, 'Data Analyst')\n",
    "business_analyst_skills = get_role_skills(job_skills_df, 'Business Analyst')\n",
    "\n",
    "print(f\"âœ… Data Scientist skills collected: {len(data_scientist_skills)}\")\n",
    "print(f\"âœ… Data Analyst skills collected: {len(data_analyst_skills)}\")\n",
    "print(f\"âœ… Business Analyst skills collected: {len(business_analyst_skills)}\")\n",
    "\n",
    "# --- 4ï¸âƒ£ Create combined vocabulary ---\n",
    "all_skills = sorted(set(\n",
    "    list(curriculum_df['skill_normalized'].unique()) +\n",
    "    data_scientist_skills + data_analyst_skills + business_analyst_skills\n",
    "))\n",
    "\n",
    "# --- 5ï¸âƒ£ Build curriculum vectors for universities ---\n",
    "universities = curriculum_df['university'].unique()\n",
    "university_vectors = {}\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = curriculum_df[curriculum_df['university'] == uni]['skill_normalized'].tolist()\n",
    "    uni_vector = np.array([1 if skill in uni_skills else 0 for skill in all_skills])\n",
    "    university_vectors[uni] = uni_vector\n",
    "\n",
    "# --- 6ï¸âƒ£ Function to compute similarity for a given role ---\n",
    "def compute_similarity(role_skills, role_name):\n",
    "    role_vector = np.array([1 if skill in role_skills else 0 for skill in all_skills])\n",
    "\n",
    "    scores = {}\n",
    "    for uni, uni_vector in university_vectors.items():\n",
    "        sim = cosine_similarity([uni_vector], [role_vector])[0][0]\n",
    "        scores[uni] = sim\n",
    "\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['University', f'Similarity_with_{role_name.replace(\" \", \"_\")}'])\n",
    "    df = df.sort_values(by=df.columns[1], ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_name = f\"{role_name.replace(' ', '_')}_Curriculum_Alignment.csv\"\n",
    "    df.to_csv(csv_name, index=False)\n",
    "\n",
    "    print(f\"\\n=== Curriculum vs {role_name} Skill Alignment ===\")\n",
    "    print(df)\n",
    "    print(f\"âœ… Saved as '{csv_name}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- 7ï¸âƒ£ Run for all three roles ---\n",
    "df_ds = compute_similarity(data_scientist_skills, 'Data Scientist')\n",
    "df_da = compute_similarity(data_analyst_skills, 'Data Analyst')\n",
    "df_ba = compute_similarity(business_analyst_skills, 'Business Analyst')\n",
    "\n",
    "# --- 8ï¸âƒ£ Combine for summary ---\n",
    "summary_df = df_ds.merge(df_da, on='University').merge(df_ba, on='University')\n",
    "summary_df.to_csv(\"AllRoles_Curriculum_Alignment.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Combined alignment saved as 'AllRoles_Curriculum_Alignment.csv'\")\n",
    "\n",
    "# --- 9ï¸âƒ£ Optional Visualization ---\n",
    "summary_df.plot(\n",
    "    x='University',\n",
    "    kind='bar',\n",
    "    figsize=(12,6),\n",
    "    title='Curriculum Alignment with Job Roles',\n",
    "    color=['#4C72B0', '#55A868', '#C44E52']\n",
    ")\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.legend(['Data Scientist', 'Data Analyst', 'Business Analyst'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('top10us.png')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p81PJfT2-xj5"
   },
   "source": [
    "**Employability Index Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdcyiHfK-5V6"
   },
   "outputs": [],
   "source": [
    "# --- Composite Employability Index Calculation ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your combined alignment file (already created previously)\n",
    "alignment_df = pd.read_csv(\"AllRoles_Curriculum_Alignment.csv\")\n",
    "\n",
    "# Load the curriculum and job skills data again\n",
    "curriculum_df = pd.read_csv(\"skills_from_curriculum (1).csv\")\n",
    "job_skills_df = pd.read_csv(\"step3_job_skills_by_role.csv\")\n",
    "\n",
    "# Normalize skill names\n",
    "import re\n",
    "def normalize_skill(skill):\n",
    "    skill = str(skill).lower()\n",
    "    skill = re.sub(r'[^a-z0-9\\s]', ' ', skill)\n",
    "    skill = re.sub(r'\\s+', ' ', skill)\n",
    "    return skill.strip()\n",
    "\n",
    "curriculum_df['skill_normalized'] = curriculum_df['skill'].apply(normalize_skill)\n",
    "job_skills_df['skill_normalized'] = job_skills_df['job_skills'].apply(normalize_skill)\n",
    "\n",
    "# --- Step 1ï¸âƒ£: Combine all job skills and count frequency ---\n",
    "skill_demand = job_skills_df['skill_normalized'].value_counts().reset_index()\n",
    "skill_demand.columns = ['skill', 'count']\n",
    "\n",
    "# --- Step 2ï¸âƒ£: Compute Coverage Ratio and Weighted Demand Match ---\n",
    "universities = curriculum_df['university'].unique()\n",
    "\n",
    "coverage_data = []\n",
    "\n",
    "for uni in universities:\n",
    "    uni_skills = set(curriculum_df[curriculum_df['university'] == uni]['skill_normalized'])\n",
    "\n",
    "    # Total job skills\n",
    "    total_job_skills = set(skill_demand['skill'])\n",
    "\n",
    "    # Coverage ratio (how many job skills the university teaches)\n",
    "    coverage_ratio = len(uni_skills.intersection(total_job_skills)) / len(total_job_skills)\n",
    "\n",
    "    # Weighted demand match (how many high-demand skills are covered, weighted by frequency)\n",
    "    matched_skills = skill_demand[skill_demand['skill'].isin(uni_skills)]\n",
    "    weighted_match = matched_skills['count'].sum() / skill_demand['count'].sum()\n",
    "\n",
    "    coverage_data.append([uni, coverage_ratio, weighted_match])\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_data, columns=['University', 'Coverage_Ratio', 'Weighted_Match'])\n",
    "\n",
    "# --- Step 3ï¸âƒ£: Merge with similarity scores ---\n",
    "merged = alignment_df.merge(coverage_df, on='University', how='left')\n",
    "\n",
    "# Compute average similarity across all three roles\n",
    "merged['Avg_Similarity'] = merged[['Similarity_with_Data_Scientist',\n",
    "                                   'Similarity_with_Data_Analyst',\n",
    "                                   'Similarity_with_Business_Analyst']].mean(axis=1)\n",
    "\n",
    "# --- Step 4ï¸âƒ£: Calculate Employability Index ---\n",
    "merged['Employability_Index'] = (\n",
    "    0.5 * merged['Avg_Similarity'] +\n",
    "    0.3 * merged['Coverage_Ratio'] +\n",
    "    0.2 * merged['Weighted_Match']\n",
    ")\n",
    "\n",
    "# --- Step 5ï¸âƒ£: Sort and Display ---\n",
    "merged = merged.sort_values('Employability_Index', ascending=False)\n",
    "print(\"\\n=== Composite Employability Index (Higher = Better Alignment) ===\")\n",
    "print(merged[['University', 'Avg_Similarity', 'Coverage_Ratio', 'Weighted_Match', 'Employability_Index']])\n",
    "\n",
    "# --- Step 6ï¸âƒ£: Save to CSV ---\n",
    "merged.to_csv(\"Composite_Employability_Index.csv\", index=False)\n",
    "print(\"\\nâœ… Saved as 'Composite_Employability_Index.csv'\")\n",
    "\n",
    "# --- Step 7ï¸âƒ£: Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(merged['University'], merged['Employability_Index'], color='teal')\n",
    "plt.xlabel(\"Employability Index Score\")\n",
    "plt.title(\"Composite Employability Index by University\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n7asCSXYP0U"
   },
   "source": [
    "# **Topic Modelling using LDA**\n",
    "**Heatmap Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hznDBPqgYTwR"
   },
   "outputs": [],
   "source": [
    "# --- ðŸ”¹ Step 1: Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- ðŸ”¹ Step 2: Load your datasets ---\n",
    "curr_df = pd.read_csv(\"skills_from_curriculum (1).csv\")\n",
    "jobs_df = pd.read_csv(\"step3_job_skills_by_role.csv\")\n",
    "\n",
    "# Label sources\n",
    "curr_df[\"source\"] = \"Curriculum\"\n",
    "jobs_df[\"source\"] = \"Job\"\n",
    "\n",
    "# --- ðŸ”¹ Step 3: Combine into one dataframe ---\n",
    "combined_df = pd.concat([\n",
    "    curr_df[[\"university\", \"skill\", \"source\"]],\n",
    "    jobs_df.assign(university=\"Industry\")[[\"university\", \"job_skills\", \"source\"]].rename(columns={\"job_skills\": \"skill\"})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Clean text\n",
    "combined_df[\"skill\"] = combined_df[\"skill\"].fillna(\"\").str.lower()\n",
    "\n",
    "# --- ðŸ”¹ Step 4: Convert skills to Bag-of-Words matrix ---\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(combined_df[\"skill\"])\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(f\"âœ… Vocabulary size: {len(words)}\")\n",
    "\n",
    "# --- ðŸ”¹ Step 5: Fit LDA ---\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=12,       # try 8â€“10 for balanced detail\n",
    "    random_state=42,\n",
    "    learning_method=\"batch\",\n",
    "    max_iter=30\n",
    ")\n",
    "lda.fit(X)\n",
    "\n",
    "# --- ðŸ”¹ Step 6: Extract top words per topic ---\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics[f\"Topic_{topic_idx+1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "topics_dict = get_top_words(lda, words)\n",
    "print(\"\\n=== ðŸ”¹ Top Words per Topic ===\")\n",
    "for t, w in topics_dict.items():\n",
    "    print(f\"{t}: {', '.join(w)}\")\n",
    "\n",
    "# --- ðŸ”¹ Step 7: Create topic-word probability DataFrame ---\n",
    "topic_word_matrix = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "topic_word_df = pd.DataFrame(topic_word_matrix.T, columns=[f\"Topic_{i+1}\" for i in range(lda.n_components)])\n",
    "topic_word_df.insert(0, \"word\", words)\n",
    "topic_word_df.to_csv(\"LDA_Topic_Word_Matrix.csv\", index=False)\n",
    "\n",
    "# --- ðŸ”¹ Step 8: Get skill-topic probabilities ---\n",
    "doc_topic_matrix = lda.transform(X)\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "doc_topic_df = pd.DataFrame(doc_topic_matrix, columns=topic_cols)\n",
    "\n",
    "combined_topic_df = pd.concat([combined_df.reset_index(drop=True), doc_topic_df], axis=1)\n",
    "combined_topic_df.to_csv(\"LDA_Skill_Topic_Assignment.csv\", index=False)\n",
    "print(\"âœ… Saved: 'LDA_Skill_Topic_Assignment.csv'\")\n",
    "\n",
    "# --- ðŸ”¹ Step 9: Average topic weights by source (Curriculum vs Jobs) ---\n",
    "avg_by_source = combined_topic_df.groupby(\"source\")[topic_cols].mean().T\n",
    "avg_by_source[\"Difference\"] = avg_by_source[\"Job\"] - avg_by_source[\"Curriculum\"]\n",
    "avg_by_source = avg_by_source.sort_values(\"Difference\", ascending=False)\n",
    "print(\"\\n=== ðŸ”¹ Topic Weights by Source ===\")\n",
    "print(avg_by_source.round(3))\n",
    "\n",
    "# --- ðŸ”¹ Step 10: University-wise topic alignment ---\n",
    "uni_topic = combined_topic_df.groupby(\"university\")[topic_cols].mean()\n",
    "uni_topic = uni_topic.sort_index()\n",
    "\n",
    "# Normalize each universityâ€™s topic weights (optional)\n",
    "uni_topic_norm = uni_topic.div(uni_topic.sum(axis=1), axis=0)\n",
    "uni_topic_norm.to_csv(\"University_Topic_Alignment.csv\")\n",
    "print(\"\\nâœ… Saved: 'University_Topic_Alignment.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj98Jn4EYPi4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1ï¸âƒ£ Replace this with your real data (10 universities Ã— 8 topics) ---\n",
    "data = {\n",
    "    'University': [\n",
    "        'Carnegie Mellon (Tepper)',\n",
    "        'MIT (Sloan)',\n",
    "        'Columbia University',\n",
    "        'New York University (Stern)',\n",
    "        'UT Austin (McCombs)',\n",
    "        'UCLA (Anderson)',\n",
    "        'USC (Marshall)',\n",
    "        'Northwestern (Kellogg)',\n",
    "        'University of Chicago (Booth)',\n",
    "        'Duke University (Fuqua)'\n",
    "    ],\n",
    "    'Topic_1': [0.134, 0.102, 0.110, 0.125, 0.118, 0.107, 0.120, 0.111, 0.106, 0.112],\n",
    "    'Topic_2': [0.163, 0.148, 0.150, 0.158, 0.160, 0.153, 0.155, 0.150, 0.157, 0.149],\n",
    "    'Topic_3': [0.176, 0.160, 0.155, 0.164, 0.170, 0.158, 0.162, 0.166, 0.168, 0.159],\n",
    "    'Topic_4': [0.092, 0.081, 0.078, 0.090, 0.088, 0.080, 0.083, 0.084, 0.085, 0.081],\n",
    "    'Topic_5': [0.075, 0.085, 0.079, 0.080, 0.082, 0.078, 0.079, 0.081, 0.083, 0.080],\n",
    "    'Topic_6': [0.125, 0.122, 0.118, 0.130, 0.122, 0.119, 0.120, 0.124, 0.126, 0.121],\n",
    "    'Topic_7': [0.115, 0.108, 0.105, 0.110, 0.112, 0.106, 0.107, 0.108, 0.111, 0.109],\n",
    "    'Topic_8': [0.121, 0.119, 0.117, 0.123, 0.125, 0.118, 0.119, 0.120, 0.124, 0.122]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('University', inplace=True)\n",
    "\n",
    "# --- 2ï¸âƒ£ Optional: Rename topics to meaningful labels ---\n",
    "df.columns = [\n",
    "    'Optimization & Stats',\n",
    "    'Core ML & Modeling',\n",
    "    'Applied ML Workflows',\n",
    "    'NLP & Cloud Processing',\n",
    "    'Analytics Tools',\n",
    "    'Forecasting & Big Data',\n",
    "    'Cloud & Programming',\n",
    "    'Programming & Databases'\n",
    "]\n",
    "\n",
    "# --- 3ï¸âƒ£ Absolute Heatmap (Actual Topic Weights) ---\n",
    "plt.figure(figsize=(13,7))\n",
    "sns.heatmap(df, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", linewidths=0.5, cbar_kws={'label': 'Topic Weight'})\n",
    "plt.title(\"Curriculum Topic Distribution Across 10 Universities\", fontsize=15, fontweight='bold', pad=20)\n",
    "plt.xlabel(\"Skill Topic Cluster\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"University\", fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap')\n",
    "plt.show()\n",
    "\n",
    "# --- 4ï¸âƒ£ Relative Difference Heatmap (Highlight Gaps vs Average) ---\n",
    "df_diff = df - df.mean()\n",
    "\n",
    "plt.figure(figsize=(13,7))\n",
    "sns.heatmap(df_diff, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", center=0, linewidths=0.5,\n",
    "            cbar_kws={'label': 'Deviation from Average'})\n",
    "plt.title(\"University Topic Emphasis vs. Peer Average\", fontsize=15, fontweight='bold', pad=20)\n",
    "plt.xlabel(\"Skill Topic Cluster\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"University\", fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
